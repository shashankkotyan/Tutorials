{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./assets/icons/ai.png\" style=\"height:1em\"> Hands on learning of Deep Learning and Neural Networks <img src=\"./assets/icons/ai-assistant.png\" style=\"height:1em\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/footsteps.png\" style=\"height:1em\"> Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Story.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "**Deep learning (DL)** is a subset of *artificial intelligence (AI)* and *machine learning (ML)* that uses multi-layered *artificial neural networks* to deliver state-of-the-art accuracy in tasks like object detection, speech recognition, language translation, and others.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/DL.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "The word \"*deep*\" in deep learning represents the many layers of algorithms, or neural networks, that are used to **recognize patterns** in data. \n",
    "Deep Learning's highly flexible architectures can learn directly from raw data, similar to the way the human brain operates, and can increase their predictive accuracy when provided with more data.   \n",
    "\n",
    "Further, deep learning is the primary technology that allows *high precision* and accuracy in tasks such as speech recognition, language translation, and object detection. \n",
    "It has led to many recent breakthroughs in Artificial Intelligence, including Open AI's ChatGPT, Google DeepMindâ€™s AlphaGo, Self-driving cars, Intelligent voice assistants (Siri, Cortana, and Alexa), and many others.\n",
    "Many experiments have shown that neural networks are particularly good with natural data (speech, vision, language) which exhibit highly nonlinear properties.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Applications.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Quiz-AI.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "This tutorial will help you understand the basics of neural networks from the perspective of optimization. \n",
    "\n",
    "At the end of the tutorial, you will have an understanding on, \n",
    "1. Gradient Descent Algorithm for Optimization\n",
    "2. Modelling Simple Multi-Layer Perceptrons\n",
    "3. Backpropogation Algorithm\n",
    "4. Training your own simple neural network for classification\n",
    "5. (Bonus) Fooling a trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <img src=\"./assets/icons/setup.png\" style=\"height:1em\"> Let's setup by importing relevant libraries and relevant utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we import the relevant libraries. \n",
    "\n",
    "**Note**: Graphviz may require manual installation, see this webpage for more information (https://graphviz.org/download/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install graphviz\n",
    "%pip install scikit-learn\n",
    "%pip install adjustText\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from graphviz import Digraph\n",
    "from adjustText import adjust_text\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/thinking.png\" style=\"height:1em\"> Recap on Optimization: Playing with optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap on our optimization class. \n",
    "\n",
    "Suppose we have an optimization function that looks like this, \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/images/Optimization.png\" style=\"height: 500px\"/>\n",
    "</div>\n",
    "\n",
    "We want to create an **algorithm** that can find the *maxima* (or *minima*) of the given function, i.e., the *reddest* (or *bluest*) point in the function. \n",
    "\n",
    "Let's start by creating this toy function and visualising it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a grid of Xs and Ys\n",
    "resolution = 100\n",
    "X, Y = np.meshgrid( np.linspace(-1,1,resolution), np.linspace(-1,1,resolution) )\n",
    "\n",
    "# Defining 4 different 2D functions\n",
    "mux, muy, sigma = 0.3, -0.3, 4\n",
    "G1 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = -0.3, 0.3, 2\n",
    "G2 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = 0.6, 0.6, 2\n",
    "G3 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux ,muy, sigma = -0.4, -0.2, 3\n",
    "G4 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "\n",
    "# Composing the final function\n",
    "G = G1 + G2 - G3 - G4\n",
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(6*4,6)) # Defining the figure space\n",
    "axes = fig.subplots(1, 4)          # Defining the subplots in the figure\n",
    "\n",
    "for ax, g, t in zip(axes.flat, [G1, G2, G3, G4], ['G1', 'G2', 'G3', 'G4']): # Iterating over axes and functions\n",
    "    ax.imshow(g, vmin=-1, vmax=1, cmap='jet')                               # Ploting the function on the subplot\n",
    "    ax.set(title=t, xlim=(0, 100), ylim=(0, 100))                           # Setting the title and limits of the subplot\n",
    "\n",
    "fig.tight_layout() # Removes extra spacing from the figure\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cax = ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "ax.set(title=\"Function\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.colorbar(cax) # Attaching the colorbar to the figure\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()               # Instruct Matplotlib to show the figures created\n",
    "\n",
    "# fig.savefig(\"./assets/images/Optimization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the function, so we can start optimization on the function. \n",
    "\n",
    "Let's **start** at a point, (70.0, 60.0) on the grid.\n",
    "We will sample points around this region and calulate the direction of further sampled points using the direction of movement from gradient.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5     # Number of Steps to take for optimisation\n",
    "alpha  = 0.03  # Learning rate of the optimisation\n",
    "\n",
    "w = np.array([70.0, 60.0]) # Starting Parameter (Point)\n",
    "sigma  = 3                 # Standard deviation of the samples around current parameter vector\n",
    "\n",
    "fig  = plt.figure( figsize=(5*n_iter, 5) )\n",
    "axes = fig.subplots(1, n_iter) \n",
    "\n",
    "prevx, prevy = [], []\n",
    "for q, ax in zip(range(n_iter), axes):\n",
    "    \n",
    "    # Draw the Optimization Landscape\n",
    "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "\n",
    "    # Sample Random Population\n",
    "    noise = np.random.randn(200, 2)\n",
    "    wp = np.expand_dims(w, 0) + sigma * noise\n",
    "    x,y = zip(*wp)\n",
    "    \n",
    "    # Estimate Gradient (Direction)\n",
    "    R  = np.array([G[int(wi[1]), int(wi[0])] for wi in wp])\n",
    "    R -= R.mean()\n",
    "    R /= R.std() \n",
    "    g  = np.dot(R, noise)\n",
    "    u  = alpha * g\n",
    "    \n",
    "    prevx.append(w[0])\n",
    "    prevy.append(w[1])\n",
    "    \n",
    "    # Draw Population on Landscape (Black Points)\n",
    "    ax.scatter(x, y, 4, 'k', edgecolors='face')\n",
    "    \n",
    "    # Draw estimated gradient (direction) as arrow (White Arrow)\n",
    "    ax.arrow(w[0], w[1], u[0], u[1], head_width=3, head_length=5, fc='w', ec='w')\n",
    "    \n",
    "    # Draw Parameter History (White Points)\n",
    "    ax.plot(prevx, prevy, 'wo-')\n",
    "    \n",
    "    # Update Parameter According to the gradient\n",
    "    w += alpha * g\n",
    "    \n",
    "    ax.set(title=f\"Iteration: {q+1} | Reward: {G[int(w[0]), int(w[1])]:.2f}\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try optimizing for finding minima (bluest part) from the function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/fast.png\" style=\"height:1em\"> Playing with gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by creating our own **datatype** to save a *single scalar value* and its *gradient*. \n",
    "\n",
    "**Note**: We are creating our own datatype just to understand the behind the scenes working of well established libraries like, Numpy, Tensorflow, PyTorch, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, label='', _children=(), _op=''):\n",
    "        \n",
    "        # Information about value, gradient and its name\n",
    "        self.data  = data\n",
    "        self.grad  = 0.0\n",
    "        self.label = label\n",
    "        \n",
    "        # Utility attributes for the calculating and passing gradients (Backprop)\n",
    "        self._backward = lambda: None\n",
    "        self._prev     = set(_children)\n",
    "        self._op       = _op \n",
    "    \n",
    "    # Simple arithemtic operations on value and computing corresponding gradients   \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, label='+', _children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, label='*', _children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad += self.data  * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, label=f'**{other}', _children=(self,), _op='**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Other arithmetic operations\n",
    "    ### Don't need to define backward functions since, they use __mul__ or __add__ for which backward is already defined. \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    # Simple transformations on Value and computing corresponding gradients\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, label='ReLU', _children=(self,), _op='ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, label='Tanh', _children=(self, ), _op='Tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "  \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), label='Exp',  _children=(self, ), _op='Exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Information when printing instance\n",
    "    def __repr__(self):\n",
    "        if self.label:\n",
    "            return f\"Value(node={self.label}, data={self.data}, grad={self.grad})\"\n",
    "        else:\n",
    "            return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # Recurisvely call backward -> Backprop\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some visualisation utilities which will help us understand the flow of gradients and data in a complicated fucntion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the graph from a root node\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "# Visualizes the graph built from root node\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining our own simple one-variable function and visualise it by plotting it,\n",
    "\n",
    "$$ f(x) = y = x^2 - 4x + 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x):\n",
    "    return x**2 - 4*x + 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-7, 15, 100)\n",
    "Y = cost_function(X)\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(15.0, label='X')\n",
    "y = cost_function(x)\n",
    "\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know what is the gradient for the function:\n",
    "$$ \\frac{d(f(x))}{dx} = \\frac{dy}{dx} = 2x -4 $$ \n",
    "\n",
    "However, let's use a magic function of backprop where we do not explicitly calculate the gradient and let's see how it calculates the gradient. \n",
    "\n",
    "We know, that the gradient of the function should be 26 when $x$ = 15.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the information from the gradient to update the x.\n",
    "Let's also visualise to understand what is happening with the x over the iterations.\n",
    "\n",
    "Can we find the minima of the function by just **iteratively** doing this updation?\n",
    "\n",
    "Rule for Gradient Descent update:\n",
    "\n",
    "$$ w^{(k+1)} = w^{(k)} - \\alpha \\nabla_w \\mathcal{J}(w^{(k)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3 # 0.1 # 0.3 # 0.75 #\n",
    "num_iterations = 10\n",
    "\n",
    "x = Value(15.0, label='X')\n",
    "\n",
    "xy_list = []\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Calculate f(x)\n",
    "    y = cost_function(x)\n",
    "    \n",
    "    # Calculate dy/dx\n",
    "    y.backward()\n",
    "    \n",
    "    xy_list.append((x.data, y.data))\n",
    "    print(f\"Step: {i+1:2d} | X: {x.data:5.2f} | f(X): {y.data:8.4f} | Gradient dy/dx: {x.grad:7.4f}\")\n",
    "    \n",
    "    # Update x \n",
    "    x -= alpha * x.grad\n",
    "\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "\n",
    "xy_list = np.asarray(xy_list)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax  = fig.subplots()\n",
    "\n",
    "ax.plot(X, Y)\n",
    "ax.plot(xy_list[:, 0], xy_list[:, 1], 'r--', marker=\"o\")\n",
    "\n",
    "texts = []\n",
    "for i in range(len(xy_list)):\n",
    "    text = ax.text(xy_list[i, 0], xy_list[i, 1], f\"({i+1}, {round(xy_list[i, 0], 2)}, {round(xy_list[i, 1], 4)})\", ha='center', va='center')\n",
    "    texts.append(text)\n",
    "adjust_text(texts, expand=(3, 3.5), arrowprops=dict(arrowstyle='->', color='grey'))\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Cost Function', title=f\"$f(x) = y = x^2 - 4x + 3$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try some 2D function and see how it goes for the minima. \n",
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Can you guess how gradients will be calculated manually for multivariate functions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/classification.png\" style=\"height:1em\"> Moving towards Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done a **regression task**, where we ride along the function to find the minima. \n",
    "\n",
    "However, what should be done when we want to perform **classification**?\n",
    "\n",
    "Moreover, what do we do when we have an approximation of the function defined by **sampled points**? \n",
    "\n",
    "Let's first visualise what I am talking about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = datasets.make_moons(n_samples=100, noise=0.1)\n",
    "X_test,  y_test  = datasets.make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "# make y be -1 or 1\n",
    "y_train = y_train*2 - 1 \n",
    "y_test  = y_test*2 - 1\n",
    " \n",
    "cmap = plt.cm.Spectral\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=[cmap(i%200) for i in y_train],               s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=[cmap(i%200) for i in y_test], c='w', s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the above figure, we don't have an exact defined function for either *left-moon* or *right-moon*. Moreover, we have to separate the two moons rather than finding a minima. \n",
    "\n",
    "<br>\n",
    "\n",
    "This is the task, which the current deep learning networks or artificial neural networks can do the best! \n",
    "\n",
    "Let's learn about the neural networks, which build up the field of deep learning! \n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/ML-DL.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "Deep learning is a subset of machine learning, with the difference that deep learning algorithms can automatically learn representations from data such as images, video, or text, without introducing human domain knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/lab.png\" style=\"height:1em\"> Microscopic view of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/neuron.png\" style=\"height:1em\"> Visualising Artificial Neuron (Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Neuron.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "Feed-Forward Neural Networks are inspired by the information processing of one or more neural cells, called a **neuron**. \n",
    "In our human brain, a *neuron* accepts input signals via its dendrites, which pass the electrical signal down to the cell body. \n",
    "The axon carries the signal out to synapses, which are the connections of a cellâ€™s axon to other cellâ€™s dendrites.\n",
    "\n",
    "<details>\n",
    "<br>\n",
    "The human nervous system is composed of more than 100 billion cells known as neurons. \n",
    "\n",
    "A neuron is a cell in the nervous system whose function it is to receive and transmit information. \n",
    "\n",
    "Neurons are made up of three major parts:\n",
    "\n",
    "* The cell body, or **soma**, which contains the nucleus of the cell and keeps the cell alive\n",
    "* A branching treelike fiber known as the **dendrite**, which collects information from other cells and sends the information to the soma\n",
    "* A long, segmented fiber known as the **axon**, which transmits information away from the cell body toward other neurons or to the muscles and glands\n",
    "\n",
    "<img src=\"https://c4.staticflickr.com/3/2656/4253587827_9723c3ffd3_z.jpg\" />\n",
    "\n",
    "*Photo courtesy of GE Healthcare, http://www.flickr.com/photos/gehealthcare/4253587827/ *\n",
    "\n",
    "<img src=\"https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg\"/>\n",
    "\n",
    "Some neurons have hundreds or even thousands of dendrites, and these dendrites may themselves be branched to allow the cell to receive information from thousands of other cells. \n",
    "\n",
    "The axons are also specialized; some, such as those that send messages from the spinal cord to the muscles in the hands or feet, may be very long---even up to several feet in length. \n",
    "To improve the speed of their communication, and to keep their electrical charges from shorting out with other neurons, axons are often surrounded by a **myelin sheath**. \n",
    "The myelin sheath is a layer of fatty tissue surrounding the axon of a neuron that both acts as an insulator and allows faster transmission of the electrical signal.\n",
    "Axons branch out toward their ends, and at the tip of each branch is a *terminal button*.\n",
    "</details>\n",
    "\n",
    "The actual working of neurons involves many aspects (including chemical, electrical, physical, timings). \n",
    "\n",
    "We will abstract all of this away into three numbers:\n",
    "\n",
    "* **Activation** - A value representing the excitement of a neuron\n",
    "* **Bias** - A value representing a default or bias (sometimes called a threshold)\n",
    "* **Weight** - A value representing a connection to another neuron\n",
    "\n",
    "In addition, there is a **transfer function** that takes all of the incoming activations times their associated weights plus the bias, and squashes the resulting sum. \n",
    "This limits the activations from growing too big or too small.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/images/Perceptron.png\"/>\n",
    "</div>\n",
    "\n",
    "A perceptron maintains an activation value that depends on: \n",
    "a) the activation values of its incoming neighbors, \n",
    "b) the weights from its incoming neighbors, and \n",
    "c) an additional value, called the **default bias value**. \n",
    "To compute this activation value, we first calculate the neuron's net input.\n",
    "\n",
    "The net input is a weighted sum of all the incoming activations plus the neuron's bias value:\n",
    "\n",
    "$$ y = f(b + \\sum\\limits_{i=1}^n x_i w_i) $$\n",
    "\n",
    "where $w_{i}$ is the weight, or connection strength, from the $i^{th}$ neuron, $x_i$ is the  input, $b$ is the bias value, and $f$ is the activation function that transforms the linear combination of inputs and weights. \n",
    "\n",
    "In a nutshell, a perceptron transforms input data by applying a nonlinear function to a weighted sum of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# Weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# Bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, single perceptrons are often used as building blocks for more complex models, such as multi-layer perceptrons (MLPs), which can handle a wider range of problems. The basic perceptron model can only be used on classification problems that are linearly separable.  \n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Quiz-Perceptron.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/deep-learning.png\" style=\"height:1em\"> Network of Neurons (Multi Layer Perceptron --- MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a network of neurons, we first start by grouping neurons together in **layers**.\n",
    "\n",
    "A typical **Artificial Neural Network (ANN)** is composed of three layers: **input**, **hidden**, and **output**. Each layer contains a collection of neurons. Typically, the neurons in a layer are **fully connected** to the neurons in the next layer. \n",
    "\n",
    "For instance, every input neuron will have a weighted connection to every hidden neuron. Similarly, every hidden neuron will have a *weighted connection* to every output neuron.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/ANN.png\"/>\n",
    "    <br>\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/NeuralNetwork.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "Processing in a network works as follows:\n",
    "\n",
    "Input is propagated forward from the input layer through the hidden layer and finally through the output layer to produce a response. Each neuron, regardless of the layer it is in, uses the same transfer function in order to propagate its information forward to the next layer. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/ForwardPropagation.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "The output of hidden (intermediate) layer is called **features**. The neural network learns multiple layers of *nonlinear features* (like edges and shapes) through repeated transformations, which it then combines in a final layer to create a prediction (of more complex objects). \n",
    "\n",
    "This makes neural networks a general non-linear classifier, as opposed to perceptron which is a linear classifier. \n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Quiz-NN.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "Let's now create a neuron, a layer and a network of neurons using our defined Value class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    # Explictly make gradients 0.0\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    # List of Parameters\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # Initialises weights, bias and activations for the neuron\n",
    "    def __init__(self, nin, activation='ReLU', layer_name='', neuron_name=''):\n",
    "        \n",
    "        self.w = [Value(np.random.uniform(-1,1), label=f\"Weight of {layer_name} {neuron_name} for Input {i+1}\") for i in range(nin)]\n",
    "        self.b = Value(0, label=f\"Bias of {layer_name} {neuron_name}\")\n",
    "        self.activation = activation\n",
    "\n",
    "    # Sets the list of parameters in the neuron\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    # Information when printing neuron\n",
    "    def __repr__(self):\n",
    "        return f\"{self.activation}Neuron(nin={len(self.w)})\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the neuron\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        w = sum((wi*xi for wi,xi in zip(self.w, x)))\n",
    "        out = w + self.b\n",
    "        \n",
    "        if self.activation == 'ReLU':\n",
    "            out = out.relu()\n",
    "        elif self.activation == 'Tanh':\n",
    "            out = out.tanh()\n",
    "        elif self.activation == 'Linear':\n",
    "            out = out\n",
    "            \n",
    "        return out\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        # Define neurons of a layer\n",
    "        self.neurons = [Neuron(nin, neuron_name=f\"Neuron {i+1}\", **kwargs) for i in range(nout)]\n",
    "\n",
    "    # Sets the list of parameters in the layer\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    # Information when printing layer\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [ {', '.join(str(n) for n in self.neurons)} ]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the layer\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts, activations=None):\n",
    "        if activations is not None:\n",
    "            assert len(nouts) == len(activations), 'Activations not defined for some layers'\n",
    "        else:\n",
    "            activations = ['Linear'] * len(nouts)\n",
    "            \n",
    "        sz = [nin] + nouts \n",
    "        \n",
    "        # Define layers of a MLP\n",
    "        self.layers = [Layer(sz[i], sz[i+1], activation=activations[i], layer_name=f\"Layer {i+1}\") for i in range(len(nouts))]\n",
    "\n",
    "    # Sets the list of parameters in the MLP\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    # Information when printing MLP\n",
    "    def __repr__(self):\n",
    "        new_line = f\"\\n{'-'*8}> \"\n",
    "        return f\"MLP of [{new_line}{new_line.join(str(layer) for layer in self.layers)}\\n]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the MLP\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function that will compute **loss** *(reward/penalty)* for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch_size=None, X=X_train, y=y_train):\n",
    "    \n",
    "    # Process Data in batches, in case data is too big to handle\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    \n",
    "    # Format Data to our Datatype\n",
    "    inputs = [ [Value(xrow[0], label='X'), Value(xrow[1], label='Y')] for xrow in Xb]\n",
    "    \n",
    "    # Forward Pass to get the scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Max-Margin Loss to calculate fitness based on scores and y\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    output_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # L2 Regularization (Optional)\n",
    "    ## To improve performance, we also regularise the parameters. \n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    \n",
    "    # Compute Final Loss -> Max-Margin Loss + L2 Regularization Loss\n",
    "    loss = output_loss + reg_loss\n",
    "    \n",
    "    # Compute Predictions and Accuracy\n",
    "    predictions = np.array([1 if (scorei.data > 0) else -1 for scorei in scores])\n",
    "    accuracy    = sum([(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)])/len(yb)\n",
    "    \n",
    "    # Return everything required\n",
    "    data = {}\n",
    "    data['loss']        = loss\n",
    "    data['scores']      = scores\n",
    "    data['predictions'] = predictions\n",
    "    data['accuracy']    = 100*accuracy\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(nin=2, nouts=[2, 2, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = compute_loss(model)\n",
    "print(f\"Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']: 5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/update.png\" style=\"height:1em\"> Backpropagation --- Trick to update weights (parameters) of multilayer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Backpropagation.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "For many years, it was unknown how to learn the weights in a multi-layered neural network. \n",
    "In addition, Marvin Minsky and Seymour Papert proved in their 1969 book [[Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book))] that you could not do simple functions without having multi-layers. \n",
    "\n",
    "(Actually, the idea of using simulated evolution to search for the weights could have been used, but no one thought to do that.) \n",
    "\n",
    "Specifically, they looked at the function XOR:\n",
    "\n",
    "**Input 1** | **Input 2** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 1 \n",
    " 1 | 0 | 1 \n",
    " 1 | 1 | 0 \n",
    "\n",
    "This killed research into neural networks for more than a decade. \n",
    "So, the idea of neural networks generally was ignored until the mid 1980s when the **Back-Propagation of Error** (Backprop) was created [[Rumelhart et.al.](https://dl.acm.org/doi/10.5555/65669.104451)].\n",
    "</details>\n",
    "\n",
    "The **Backpropagation algorithm** (using *Backprop*), also called the *generalized delta rule*, is a *supervised* learning method for multilayer feed-forward networks in the field of Deep Learning.\n",
    "Technically, it is a method for training the weights in a multilayer feed-forward neural network. \n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected output signal. \n",
    "The system is trained using a *supervised learning* method, where the error between the systemâ€™s output and a known expected output is presented to the system and used to modify its internal state.\n",
    "\n",
    "With backpropagation, the errors are sent back through the network again and the weights are adjusted, improving the neural network. \n",
    "The neural network learns by varying the weights or parameters of a network so as to minimize the difference between the predictions of the neural network and the desired values. \n",
    "This process is repeated thousands of times, adjusting a neural network's weights in response to the error it produces, until the error can't be reduced anymore. \n",
    "This phase where the artificial neural network learns from the data is called training.\n",
    "During this process, the layers learn the optimal features for the model, which has the advantage that features do not need to be predetermined.\n",
    "\n",
    "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scores'][0].backward()\n",
    "draw_dot(data['scores'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/practice.png\" style=\"height:1em\"> Train a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our neural network to classify the two moons. \n",
    "\n",
    "Backprop in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "n_log  = 1\n",
    "learning_rate = lr = 1.0\n",
    "\n",
    "model = MLP(nin=2, nouts=[16, 16, 1], activations=['ReLU', 'ReLU', 'Linear']) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "train_history = []\n",
    "\n",
    "# Optimize Iteratively\n",
    "for k in range(n_iter):\n",
    "    \n",
    "    # Zero-Grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward Pass -> Compute Loss\n",
    "    data = compute_loss(model)\n",
    "    \n",
    "    # Backward Pass\n",
    "    data['loss'].backward()\n",
    "        \n",
    "    # Log Details\n",
    "    if k % n_log == 0:\n",
    "        print(f\"Step: {k+1:3d} | Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']:5.2f}% | Learning Rate: {lr:.2f}\")\n",
    "        train_history.append((data['loss'].data, data['accuracy'], lr))\n",
    "    \n",
    "    # Update Weights using SGD\n",
    "    lr = learning_rate - 0.9*(k+1)/n_iter\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualise, Loss, Accuracy and Learning Rate over the iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6*3, 6))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "train_history = np.asarray(train_history)\n",
    "\n",
    "for ax, d, t in zip(axes.flat, [train_history[:, 0], train_history[:, 1], train_history[:, 2]], ['Loss', 'Accuracy', 'Learning Rate']):\n",
    "    ax.plot(d)\n",
    "    ax.set(title=t, xlim=(0, n_iter))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the predictions of the neural network and see which points are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = compute_loss(model, X=X_train, y=y_train)\n",
    "test_data  = compute_loss(model, X=X_test,  y=y_test)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the **boundary** of the trained neural network, how does the boundary separating the two moons look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Decision Boundary\n",
    "resolution = 0.25\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 \n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max+ resolution, resolution), np.arange(y_min, y_max+ resolution, resolution))\n",
    "\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "\n",
    "scores = list(map(model, inputs))\n",
    "\n",
    "Z = np.array([s.data > 0 for s in scores]).reshape(xx.shape)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.contourf(xx, yy, Z, colors=[cmap(-1%200), cmap(1)], alpha=0.25)\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(x_min, x_max), ylim=(y_min, y_max))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try some other simple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/action.png\" style=\"height:1em\"> Moving Forward: Example of Handwritten Digits Classification using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a task, how does one train a neural network to do/solve the task? This involves the following steps:\n",
    "\n",
    "1. Determine an appropriate network architecture.\n",
    "2. Define a data set that will be used for training.\n",
    "3. Define the neural network parameters to be used for training.\n",
    "4. Train the network.\n",
    "5. Test the trained network.\n",
    "6. Do post training analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/setup.png\" style=\"height:1em\"> Libraries Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We again start by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import repeat\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn, optim as optim\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = 'cuda' # 'cpu' # 'cuda'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/construction.png\" style=\"height:1em\"> Determining an appropriate architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a neural network consists of an input layer, an output layer, and zero or more hidden layers. Once a network has been trained, when you present an input to the network, the network will propagate the inputs through its layers to produce an output. \n",
    "\n",
    "If the input represents an instance of the task, the output should be the solution to that instance after the network has been trained. Thus, one can view a neural network as a general pattern associator. Thus, given a task, the first step is to identify the nature of inputs to the pattern associator. \n",
    "\n",
    "This is normally in the form of number of neurons required to represent the input. \n",
    "Similarly, you will need to determine how many output neurons will be required. \n",
    "\n",
    "For example, consider a simple logical connective, AND whose input-output characteristics are summarized in the table below:\n",
    "\n",
    "**Input A** | **Input B** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 0 \n",
    " 1 | 0 | 0 \n",
    " 1 | 1 | 1 \n",
    "\n",
    "This is a very simple example, but it will help us illustrate all of the important concepts in defining and training neural networks.\n",
    "\n",
    "In this example, it is clear that we will need two neurons in the input layer, and one in the output layer. \n",
    "We can start by assuming that we will not need a hidden layer. In general, as far as the design of a neural network is concerned, you always begin by identifying the size of the input and output layers. \n",
    "\n",
    "Then, you decide how many hidden layers you would use. \n",
    "In most situations you will need atleast one hidden layer, though there are no hard and fast rules about its size. \n",
    "Through much empirical practice, you will develop your own heuristics about this. \n",
    "\n",
    "For the MNIST Dataset, we have used 2 Convolution Layer and 2 MLP Layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/mining.png\" style=\"height:1em\"> Define a data set that will be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have decided on the network architecture, you have to prepare the data set that will be used for training. Each item in the data set represents an input pattern and the correct output pattern that should be produced by the network (since this is supervised training). \n",
    "\n",
    "In most tasks, there can be an infinite number of such input-output associations. Obviously it would be impossible to enumerate all associations for all tasks (and it would make little sense to even try to do this!). \n",
    "You have to then decide what comprises a good representative data set that, when used in training a network, would generalize to all situations.\n",
    "\n",
    "In the example of the AND, the data set is very small, finite (only 4 cases!), and **exhaustive**.\n",
    "\n",
    "However, MNIST Dataset consists of images of size 28*28 pixels which are monochromatic. Since monochromatic means either black or white, there exists $ 2^{28 \\times 28} $ possible images (this number is 237 digits long). If we go for greyscale, there would exist $ 256^{28\\times28} $ possible images (this number is 1889 digits long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 128\n",
    "test_batch_size = 512\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs  = {'batch_size': test_batch_size}\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "            \n",
    "train_dataset = datasets.MNIST('../data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test_dataset  = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader  = infinite_loader(torch.utils.data.DataLoader(train_dataset,**train_kwargs))\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(vutils.make_grid(batch[0].to(device)[:32], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/adjust.png\" style=\"height:1em\"> Define the neural network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the hyperparameters required to train the neural network.\n",
    "The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. \n",
    "If you train the neural network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. \n",
    "That is, the loss on the validation set will start increasing as the training set loss drops.\n",
    "\n",
    "#### Choose the number of iterations\n",
    "This is the number of batches of samples from the training data we'll use to train the network. The more iterations you use, the better the model will fit the data. However, if you use too many iterations, then the model with not generalize well to other data, this is called overfitting. You want to find a number here where the network has a low training loss, and the validation loss is at a minimum. As you start overfitting, you'll see the training loss continue to decrease while the validation loss starts to increase.\n",
    "\n",
    "#### Choose the learning rate\n",
    "This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. A good choice to start at is 0.1. If the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.\n",
    "\n",
    "#### Choose the number of hidden neurons\n",
    "The more hidden neurons you have, the more accurate predictions the model will make. Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden neurons is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden neurons you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter          = 200\n",
    "learning_rate   = 0.1\n",
    "n_log           = 1\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0.01, T_max=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/predictive.png\" style=\"height:1em\"> Training and Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once all the parameters are specified, you start the training process. \n",
    "This involves presenting each input pattern to the network, propagating it all the way until an output is produced, comparing the output with the desired target, computing the error, backpropagating the error, and applying the learning rule. \n",
    "This process is repeated as needed. \n",
    "\n",
    "In general, you should train the network for several iterations, it can be anywhere from a few hundred to millions! depennding on the dataset. \n",
    "Gradually, the network will begin to show improved and stable performance. \n",
    "Performance of the network is measured according to the task, for our classificiation task, we measure the accuracy, while for regression task, we can measure mean squared error. \n",
    "\n",
    "You can either stop the training process after a certain number of iterations have elapsed, or after the performance has saturated (early-stopping).\n",
    "\n",
    "Once the network has been trained, it is time to test it. \n",
    "There are several ways of doing this. \n",
    "Perhaps the easiest is to turn learning off and then see the outputs produced by the network for each input in the data set. \n",
    "When a trained network is going to be used in a *deployed* application, all you have to do is save the weights of all interconnections in the network into a file. \n",
    "The trained network can then be recreated at anytime by reloading the weights.\n",
    "\n",
    "**Note**: Instead of training-then-testing, there is another methodology: you can test-while-training, which encompases other evaluation technqiues like cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, n_iter):\n",
    "    train_history = []\n",
    "    model.train()\n",
    "    with tqdm(total=n_iter) as bar:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Explictily Zeroing Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % n_log == 0:\n",
    "                bar.update(n_log)\n",
    "                bar.set_postfix({'Loss':  f\"{loss.item():.4f}\", 'Learning Rate': f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
    "                train_history.append((loss.item, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            # Changing Learning Rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx == n_iter-1:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    \n",
    "    with torch.no_grad() and tqdm(total=len(test_loader)) as bar:\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target, reduction='sum').item()  \n",
    "            \n",
    "            # Get Prediction\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            correct   += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss += loss\n",
    "            \n",
    "            bar.update(1)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct   /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Test set--- Average loss: {test_loss:.4f}, Accuracy: {100. * correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, optimizer, scheduler, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model for reusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(model).save('mnist-try.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the saved model and evaluate it on our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.jit.load('mnist-try.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(trained_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/analytics.png\" style=\"height:1em\"> Do post training analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Perhaps the most important step in using neural networks is the analysis one performs once a network has been trained. There are a whole host of analysis techniques, here we present some of them which can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(embedding):\n",
    "    fig, ax = plt.subplots()\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    \n",
    "    for digit in range(10):\n",
    "        ax.scatter(*X[y == digit].T, marker=f\"${digit}$\", s=60, color=plt.cm.Dark2(digit), alpha=0.425, zorder=2,)\n",
    "    \n",
    "    shown_images = np.array([[1.0, 1.0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i])\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    # ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "num_images = 1000\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2)\n",
    "\n",
    "images, labels = [], []\n",
    "for i, (x, y) in enumerate(train_dataset):\n",
    "    images.append(x.numpy())\n",
    "    labels.append(y)\n",
    "    if i == num_images -1:\n",
    "        break\n",
    "    \n",
    "image_embedding = tsne.fit_transform(images, labels)\n",
    "plot_embedding(image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(test_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(torchvision.utils.make_grid(batch[0].to(device)[:16], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try more advanced adaptive optimizers like Adam \n",
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try some more innovative analysis to understand the trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/jester.png\" style=\"height:1em\"> Bonus: Fool the trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/warning.png\" style=\"height:1em\"> Run on [Google Collab](https://colab.research.google.com/) or a PC with Nvidia GPU  <img src=\"./assets/icons/warning.png\" style=\"height:1em\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/setup.png\" style=\"height:1em\"> Libraries Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install robustbench\n",
    "from scipy.optimize import differential_evolution as de\n",
    "\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/data-modeling.png\" style=\"height:1em\"> Loading Data and Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "x_test, y_test = load_cifar10(n_examples=50)\n",
    "model = load_model(model_name='Standard', dataset='cifar10', threat_model='Linf').to(device)\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 10\n",
    "image = x_test[image_id]\n",
    "label = y_test[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.softmax(model(image[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/anomaly.png\" style=\"height:1em\"> Defining Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image(perturbations, img):\n",
    "    \n",
    "    perturbations = perturbations.astype(int)\n",
    "    if perturbations.ndim < 2:\n",
    "        perturbations = np.array([perturbations])\n",
    "    \n",
    "    img = (img * 255).detach().cpu().numpy().astype(int)\n",
    "    imgs = np.tile(img, [len(perturbations)] + [1]*(perturbations.ndim+1))\n",
    "    \n",
    "    for x, img in zip(perturbations, imgs):\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            x_pos, y_pos, *value = pixel\n",
    "            img[:, round(x_pos), round(y_pos)] = value\n",
    "        \n",
    "    imgs = imgs/255.0\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = np.array([6, 6, 255, 0, 0]) \n",
    "image_perturbed = perturb_image(pixel, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/cyber-attack.png\" style=\"height:1em\"> Few-Pixel Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(image, target, pixel_count=1, maxiter=100, popsize=256):\n",
    "    \n",
    "    bounds = [(0,32), (0,32), (0,256), (0,256), (0,256)] * pixel_count\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "    \n",
    "    def run(perturbations, image, target, evaluate=False):\n",
    "        images_perturbed = perturb_image(perturbations, image).to(device)\n",
    "        probabilities = torch.softmax(model(images_perturbed), dim=1)\n",
    "        if evaluate:\n",
    "            prediction = torch.argmax(probabilities, dim=1)\n",
    "            return prediction != target\n",
    "        else:\n",
    "            confidence = probabilities[:, target].detach().cpu().numpy()\n",
    "            return confidence\n",
    "    \n",
    "    def predict_fn(xs):\n",
    "        xs = xs.transpose()\n",
    "        return run(xs, image, target, evaluate=False)\n",
    "    \n",
    "    def callback_fn(x, convergence):\n",
    "        return run(x, image, target, evaluate=True)\n",
    "\n",
    "    result = de(\n",
    "        predict_fn, bounds=bounds, maxiter=maxiter, popsize=popmul, vectorized=True,\n",
    "        recombination=1, atol=-1, callback=callback_fn, polish=False, disp=True)\n",
    "    \n",
    "    return result.x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_perturbation = attack(image, label, pixel_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_perturbed = perturb_image(attacked_perturbation, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "true_conf = output[:, label]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\\nTrue Class {classes[label]} has {true_conf.item()*100:.2f}% condifence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Instead of DE, try to use SGD to optimize the attack\n",
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Can you modify to create a targeted attack, where you can define what would be the predicted class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/owl.png\" style=\"height:1em\"> Words of Wisdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Limitations.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assists in achieving better fit and in learning complex patterns. \n",
    "Bias allows the perceptron to make adjustments to its output independently of the inputs. \n",
    "It acts like the intercept added in a linear equation, which helps the model in a way that can best fit for the given data. \n",
    "Bias also allows the network to learn and represent more complex relationships between the input and output variables.\n",
    "\n",
    "- Handling zero inputs and mitigating the problems of Vanishing Gradients. \n",
    "Activation functions introduce non-linearity into the perceptron. \n",
    "Bias helps the perceptron shift the activation function towards positive or negative side. \n",
    "By adding the bias, perceptron can train over points that do not pass through origin, thereby ensuring that a neuron can activate even when all of its input values are zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local search algorithm. \n",
    "So in general it can get stuck in local optima. \n",
    "We use examples from linear regression which is a convex optimization problem, so for a small enough learning rate, it will converge to a global optimum.\n",
    "\n",
    "- Fixed Points of Gradient Descent, i.e., values of $ w^{(k+1)} = w^{(k)} $.\n",
    "These are the stationary points where differentiation is 0, i.e., $\\nabla \\mathcal(J) = 0$. \n",
    "They maybe stable (e.g. local optima) ot unstable (e.g. saddle points).\n",
    "\n",
    "- Invariances of Gradient Descent. \n",
    "It is invariant to rigid transformations (rotation, reflection, and translation)\n",
    "\n",
    "- Batched Stocastic Gradient Descent. \n",
    "It is possible to run stochastic gradient descent where every step we touch more than one example.\n",
    "This is called minibatch stochastic gradient descent and the number of examples we look at per iterations is called the ''minibatch size''. \n",
    "(When the minibatch size is 1, we recover stochastic gradient descent.) \n",
    "In many environments, using a larger minibatch can be a good idea because the gradient is less noisy (computed as an average over the examples) and faster because matrix-vector libraries\n",
    "work better with larger matrices.\n",
    "\n",
    "- Unbiased Estimate. Stocastic Gradient Descent makes significant progress before it has even looked at all the data, this is because if we sample a training example at random SGD gives an unbiased estimate of the batched gradient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalisation (or Standardisation). \n",
    "A coomon trick when training machine learning models is to normalize or standardize the inputs to zero mean and unit variance. This ensures faster convergenve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Neural Network\n",
    "\n",
    "- Random Intialisation of Weights. \n",
    "It's important to ''**break the symmetry**'' of the neurons in the networks or, in other words, force the neurons to be different at the beginning. \n",
    "This means that itâ€™s important to *initialize* the parameters w and b randomly. \n",
    "A good method for random initialization is Gaussian random or uniform random.\n",
    "Sometimes tuning the variance of the initialization also helps.\n",
    "Also make sure that the random initialization does not ''**saturate**'' the networks. \n",
    "This means that most of the time, for your data, the values of the neurons should be between 0.2 and 0.8. \n",
    "This is because we do not want to neurons to have too many values of zeros and ones. \n",
    "When that happens, the gradient is small and thus the training is much longer.\n",
    "\n",
    "- Choosing Learning Rate.\n",
    "Picking a good learning rate Î± can be tricky. \n",
    "A large learning rate can change the parameters too aggressively or a small learning rate can change the parameters too conservatively. \n",
    "Both should be avoided, a good learning rate is one that leads to good overall improvements in the objective function. \n",
    "To select good $\\alpha$, itâ€™s also best to monitor the progress of your training. \n",
    "In many cases, a learning rate of 0.1 or 0.01 is a very good start, if you use SGD.\n",
    "For other optimizers like Adam, starting learning rate can differ. \n",
    "\n",
    "- Choosing Hyperparamters. \n",
    "Picking good hyperparameters (architectural parameters such as number of layers, number of neurons\n",
    "on each layers) for your neural networks can be difficult and is a topic of current research. \n",
    "A standard way to pick architectural parameters is via cross-validation: Keep a hold-out validation set that the training never touches. \n",
    "If the method performs well on the training data but not the validation set, then the model overfits: it has too many degrees of freedom and remembers the training cases but does not generalize to new cases. \n",
    "If the model overfits, we need to reduce the number of hidden layers or number of neurons on each hidden layer. \n",
    "If the method performs badly on the training set then the model underfits: it does not have enough degrees of freedom and we should increase the number of hidden layers or number of neurons. \n",
    "Be warned that bad performance on the training set can also mean that the learning rate is chosen poorly.\n",
    "\n",
    "- Hyperparameter Optimization. \n",
    "Picking good hyperparameters can also be automated using grid search, random search or Bayesian\n",
    "optimization. \n",
    "In grid search, every possible combination of hyperparameters will be tried and crossvalidated with a hold-out validation set. \n",
    "In case that grid search is expensive because the number of hyperparameters is large, one can try random search where hyperparameter configurations are generated and tried at random. \n",
    "Bayesian optimization looks at the performances of networks at previous hyperparameter combinations and fits a function through these points, it then picks the next combination that maximizes some utility function such as the mean plus some function of the uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory footprint\n",
    "\n",
    "- Precision of float variables. \n",
    "It is possible to use single precision (float16) for the parameters in the networks instead of double precision (float32). \n",
    "This reduces the memory footprint of the model in half and usually does not hurt the performances of the networks. \n",
    "A downside is that it is more tricky to check the correctness of the gradient using the numerical approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Experience\n",
    "\n",
    "- Monitoring. \n",
    "Have a way to monitor the progress of your training. \n",
    "Perhaps the best method is to compute the objective function on the current example or on a subset of the training data or on a held-out validation set. \n",
    "\n",
    "- Code Optimization. \n",
    "Neural networks can take a long time to train and thus itâ€™s worth spending time to optimize the code\n",
    "for speed. \n",
    "To speed up the training, make use of fast matrix vector libraries, which often provide good speed-up over na ÌˆÄ±ve implementation of matrix vector multiplication. The vectorized version of the backpropagation algorithm will come in handy in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have to take into consideration the range of each input and output value. \n",
    "Remember the activation function of a neuron is a mostly sigmoid-like-function that serves to squash all input values between 0.0 and 1.0. \n",
    "Thus, regardless of the size of each input value into a node, the output produced by each node is between 0.0 and 1.0. \n",
    "This means that all output nodes have values in that range. I\n",
    "If the task you are dealing with expects outputs between 0.0 and 1.0, then there is nothing to worry about. \n",
    "However, in most situations, you will need to *scale* the output values back to the values in the task domain. \n",
    "Same goes for ReLU, which squashes the negative range. \n",
    "\n",
    "- In reality, it is also a good idea to scale the input values from the domain into the 0.0 to 1.0 range (especially if most input values are outside the -5.0 and 5.0 range). Thus, defining a data set for training almost always requires a collection of input-output pairs, as well as scaling and unscaling operations. Luckily, for the AND task, we do not need to do any scaling, but we will see several examples of this later.\n",
    "\n",
    "- When developing NN models, itâ€™s important to compare model performance and verify the effectiveness of your attempts to improve the model. Compiling a NN generates/initializes random weights for each connection in the network. These weights are updated during training. However, the effect of random initialization is large enough to influence model performance to the tenth decimal point, hampering model comparison. Therefore, we need to â€˜controlâ€™ this random initialization such that, when you go back and forth between iterations, pre-processing or feature engineering, the results remain comparable.\n",
    "\n",
    "The learning rate, EPSILON, and the momentum constant, MOMENTUM, have to be between 0.0 and 1.0 and are critical to the overall training algorithm. \n",
    "The appropriate values of these constants are best determined by experimentation. \n",
    "Tolerance (which is also between 0.0 and 1.0) refers to the level of tolerance that is acceptable for determining correctness of the output. \n",
    "For example, if tolerance is set to 0.1, then an output value within 10% of the desired output is considered correct. \n",
    "Other training parameters generally exist to specify the reporting rate of the progress of the training, where to log such progress, etc. \n",
    "We will see specific examples of these as we start working with actual networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/thank-you.png\" style=\"height:1em\"> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [SimpliLearn - What is Artificial Network?](https://youtu.be/uMzUB89uSxU?si=KyY5C5HhoDnDlAeL)\n",
    "- [SimpliLearn - What is a Neural Network?](https://www.youtube.com/watch?v=bfmFfD2RIcg)\n",
    "- [SimpliLearn - What is Deep Learning?](https://www.youtube.com/watch?v=6M5VXKLf4D4)\n",
    "- [Andrej Karpathy - Micrograd Library](https://github.com/karpathy/micrograd)\n",
    "- [Flaticon - Icons](https://www.flaticon.com/free-icons/)\n",
    "\n",
    "<details>\n",
    "<ul>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/brain\" title=\"brain icons\">Brain icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/robot\" title=\"robot icons\">Robot icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/analysis\" title=\"analysis icons\">Analysis icons created by Uniconlabs - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/anomaly\" title=\"anomaly icons\">Anomaly icons created by Marz Gallery - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/attention\" title=\"attention icons\">Attention icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/construction\" title=\"construction icons\">Construction icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/data-modeling\" title=\"data modeling icons\">Data modeling icons created by HAJICON - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/dataset\" title=\"dataset icons\">Dataset icons created by Fragneel - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/footsteps\" title=\"footsteps icons\">Footsteps icons created by Flat Icons - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/insight\" title=\"insight icons\">Insight icons created by noomtah - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/jester\" title=\"jester icons\">Jester icons created by Eucalyp - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/machine-learning\" title=\"machine learning icons\">Machine learning icons created by mpanicon - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/malware\" title=\"malware icons\">Malware icons created by Smashicons - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/microscope\" title=\"microscope icons\">Microscope icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/moving\" title=\"moving icons\">Moving icons created by Iconjam - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/neural-network\" title=\"neural network icons\">Neural network icons created by juicy_fish - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/neuron\" title=\"neuron icons\">Neuron icons created by Soni Sokell - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/personalized\" title=\"personalized icons\">Personalized icons created by Iconic Panda - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/practice\" title=\"practice icons\">Practice icons created by Awicon - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/remember\" title=\"remember icons\">Remember icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/setup\" title=\"setup icons\">Setup icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/speed\" title=\"speed icons\">Speed icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/study\" title=\"study icons\">Study icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/thank-you\" title=\"thank you icons\">Thank you icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/try\" title=\"try icons\">Try icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/update\" title=\"update icons\">Update icons created by Freepik - Flaticon</a>\n",
    "<li> <a href=\"https://www.flaticon.com/free-icons/wise\" title=\"wise icons\">Wise icons created by Freepik - Flaticon</a>\n",
    "</ul>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/books.png\" style=\"height:1em\"> Extra Study Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [3Blue1Brown - DeepLearning](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [Nvidia - DeepLearning Blog](https://www.nvidia.com/en-us/glossary/deep-learning/)\n",
    "- [James Loy - How to build your own Neural Network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)\n",
    "- [Douglas Blank - Artificial Neural Networks](https://jupyter.brynmawr.edu/services/public/dblank/BioCS115%20Computing%20through%20Biology/2016-Spring/Notebooks/Artificial_Neural_Networks.ipynb)\n",
    "- [Fei Fei Li - Standford University CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/index.html)\n",
    "- [Quoc V. Le - A tutorial on Deep Learning](https://cs.stanford.edu/~quocle/tutorial1.pdf)\n",
    "- [Matt Mazur - A A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Lecture 1](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/slides/lec01.pdf)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Tutorial 1](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/tutorials/tut01.pdf)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Lecture 7](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/slides/lec07.pdf)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Lecture 9](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/slides/lec09.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
