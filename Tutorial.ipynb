{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./assets/icons/ai.png\" style=\"height:1em\"> Hands on learning of Deep Learning and Neural Networks <img src=\"./assets/icons/ai-assistant.png\" style=\"height:1em\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/footsteps.png\" style=\"height:1em\"> Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Story.mp4\" controls>\n",
    "</div>\n",
    "\n",
    "Deep learning (DL) is a branch of Artificial Intelligence (AI) and Machine Learning (ML) that utilizes multi-layered Artificial Neural Networks (ANNs) to achieve cutting-edge accuracy in various tasks such as object detection, speech recognition, language translation, and more.\n",
    "Numerous experiments have demonstrated that neural networks excel with natural data (speech, vision, language) that exhibit highly nonlinear properties.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/DL.mp4\" controls>\n",
    "</div>\n",
    "\n",
    "The term \"deep\" in deep learning refers to the numerous layers of algorithms, or neural networks, utilized to recognize patterns in data. \n",
    "Deep learning's highly flexible architectures can learn directly from raw data, mimicking the operation of the human brain, and can enhance their predictive accuracy with larger datasets.\n",
    "\n",
    "Moreover, deep learning is the key technology enabling high precision and accuracy in tasks such as speech recognition, language translation, and object detection. \n",
    "It has driven many recent advancements in Artificial Intelligence, including OpenAI's ChatGPT, Google DeepMindâ€™s AlphaGo, self-driving cars, and intelligent voice assistants like Siri, Cortana, and Alexa. \n",
    "Let's explore what other applications these neural networks can handle:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Applications.mp4\" controls>\n",
    "    <br>\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Quiz-AI.mp4\" controls>\n",
    "</div>\n",
    "\n",
    "This tutorial will guide you through the fundamentals of neural networks from an optimization perspective.\n",
    "\n",
    "By the end of the tutorial, you will understand:\n",
    "\n",
    "1. The Gradient Descent Algorithm for optimization\n",
    "2. Modeling simple multi-layer perceptrons\n",
    "3. The Backpropagation Algorithm\n",
    "4. Training your own simple neural network for classification\n",
    "5. (Bonus) Techniques for fooling a trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <img src=\"./assets/icons/setup.png\" style=\"height:1em\"> Let's setup by importing relevant libraries and relevant utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the relevant libraries.\n",
    "\n",
    "**Note**: Graphviz might need to be installed manually. For more information, please visit (https://graphviz.org/download/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install graphviz\n",
    "%pip install scikit-learn\n",
    "%pip install adjustText\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from graphviz import Digraph\n",
    "from adjustText import adjust_text\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/thinking.png\" style=\"height:1em\"> Recap on Optimization: Playing with optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap our optimization class.\n",
    "\n",
    "Suppose we have an optimization function that looks like this:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Optimization.png\" width=30%>\n",
    "</div>\n",
    "\n",
    "We aim to create an algorithm that can find the maxima (or minima) of the given function, i.e., the reddest (or bluest) point in the function.\n",
    "\n",
    "Let's start by creating this toy function and visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a grid of Xs and Ys\n",
    "resolution = 100\n",
    "X, Y = np.meshgrid( np.linspace(-1,1,resolution), np.linspace(-1,1,resolution) )\n",
    "\n",
    "# Defining 4 different 2D functions\n",
    "mux, muy, sigma = 0.3, -0.3, 4\n",
    "G1 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = -0.3, 0.3, 2\n",
    "G2 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = 0.6, 0.6, 2\n",
    "G3 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux ,muy, sigma = -0.4, -0.2, 3\n",
    "G4 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "\n",
    "# Composing the final function\n",
    "G = G1 + G2 - G3 - G4\n",
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(6*4,6)) # Defining the figure space\n",
    "axes = fig.subplots(1, 4)          # Defining the subplots in the figure\n",
    "\n",
    "for ax, g, t in zip(axes.flat, [G1, G2, G3, G4], ['G1', 'G2', 'G3', 'G4']): # Iterating over axes and functions\n",
    "    ax.imshow(g, vmin=-1, vmax=1, cmap='jet')                               # Ploting the function on the subplot\n",
    "    ax.set(title=t, xlim=(0, 100), ylim=(0, 100))                           # Setting the title and limits of the subplot\n",
    "\n",
    "fig.tight_layout() # Removes extra spacing from the figure\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cax = ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "ax.set(title=\"Function\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.colorbar(cax) # Attaching the colorbar to the figure\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()               # Instruct Matplotlib to show the figures created\n",
    "\n",
    "# fig.savefig(\"./assets/images/Optimization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the function, we can start optimizing it.\n",
    "\n",
    "Let's **begin** at a point, (70.0, 60.0), on the grid. We will sample points around this region and calculate the direction of further sampled points using the gradient's direction of movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5     # Number of Steps to take for optimisation\n",
    "alpha  = 0.03  # Learning rate of the optimisation\n",
    "\n",
    "w = np.array([70.0, 60.0]) # Starting Parameter (Point)\n",
    "sigma  = 3                 # Standard deviation of the samples around current parameter vector\n",
    "\n",
    "fig  = plt.figure( figsize=(5*n_iter, 5) )\n",
    "axes = fig.subplots(1, n_iter) \n",
    "\n",
    "prevx, prevy = [], []\n",
    "for q, ax in zip(range(n_iter), axes):\n",
    "    \n",
    "    # Draw the Optimization Landscape\n",
    "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "\n",
    "    # Sample Random Population\n",
    "    noise = np.random.randn(200, 2)\n",
    "    wp = np.expand_dims(w, 0) + sigma * noise\n",
    "    x,y = zip(*wp)\n",
    "    \n",
    "    # Estimate Gradient (Direction)\n",
    "    R  = np.array([G[int(wi[1]), int(wi[0])] for wi in wp])\n",
    "    R -= R.mean()\n",
    "    R /= R.std() \n",
    "    g  = np.dot(R, noise)\n",
    "    u  = alpha * g\n",
    "    \n",
    "    prevx.append(w[0])\n",
    "    prevy.append(w[1])\n",
    "    \n",
    "    # Draw Population on Landscape (Black Points)\n",
    "    ax.scatter(x, y, 4, 'k', edgecolors='face')\n",
    "    \n",
    "    # Draw estimated gradient (direction) as arrow (White Arrow)\n",
    "    ax.arrow(w[0], w[1], u[0], u[1], head_width=3, head_length=5, fc='w', ec='w')\n",
    "    \n",
    "    # Draw Parameter History (White Points)\n",
    "    ax.plot(prevx, prevy, 'wo-')\n",
    "    \n",
    "    # Update Parameter According to the gradient\n",
    "    w += alpha * g\n",
    "    \n",
    "    ax.set(title=f\"Iteration: {q+1} | Reward: {G[int(w[0]), int(w[1])]:.2f}\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try optimizing for finding minima (bluest part) from the function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/fast.png\" style=\"height:1em\"> Playing with gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the best direction to change our weight vector, which is mathematically guaranteed to be the direction of the steepest descent (as the step size approaches zero). \n",
    "This direction is related to the gradient of the cost function. \n",
    "\n",
    "For one-dimensional functions, the slope is the instantaneous rate of change of the function at any point of interest. \n",
    "The gradient is a generalization of the slope for functions that take a vector of numbers instead of a single number. \n",
    "The gradient is a vector of slopes (or derivatives) for each dimension in the input space.\n",
    "\n",
    "The mathematical expression for the derivative of a 1-D function with respect to its input is:\n",
    "\n",
    "$$ \\frac{d(f(x))}{dx} = \\lim_{h \\to 0}~ \\frac{f(x+h) - f(x)}{h} = \\lim_{h \\to 0}~ \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "When dealing with functions that take a vector of numbers, we refer to the derivatives as partial derivatives, and the gradient is simply the vector of partial derivatives for each dimension.\n",
    "\n",
    "Let's start by creating our own **datatype** to save a *single scalar value* and its *gradient*.\n",
    "\n",
    "**Note**: We are creating our own datatype to understand the behind-the-scenes workings of well-established libraries like Numpy, TensorFlow, and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, label='', _children=(), _op=''):\n",
    "        \n",
    "        # Information about value, gradient and its name\n",
    "        self.data  = data\n",
    "        self.grad  = 0.0\n",
    "        self.label = label\n",
    "        \n",
    "        # Utility attributes for the calculating and passing gradients (Backprop)\n",
    "        self._backward = lambda: None\n",
    "        self._prev     = set(_children)\n",
    "        self._op       = _op \n",
    "    \n",
    "    # Simple arithemtic operations on value and computing corresponding gradients   \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, label='+', _children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, label='*', _children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad += self.data  * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, label=f'**{other}', _children=(self,), _op='**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Other arithmetic operations\n",
    "    ### Don't need to define backward functions since, they use __mul__ or __add__ for which backward is already defined. \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    # Simple transformations on Value and computing corresponding gradients\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, label='ReLU', _children=(self,), _op='ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, label='Tanh', _children=(self, ), _op='Tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "  \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), label='Exp',  _children=(self, ), _op='Exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Information when printing instance\n",
    "    def __repr__(self):\n",
    "        if self.label:\n",
    "            return f\"Value(node={self.label}, data={self.data}, grad={self.grad})\"\n",
    "        else:\n",
    "            return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # Recurisvely call backward -> Backprop\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can store a scalar value and its gradient. Moreover, it can also compute gradients and update it accordingly.\n",
    "\n",
    "Let's create some visualization utilities to help us understand the flow of gradients and data in a complicated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the graph from a root node\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "# Visualizes the graph built from root node\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining our own simple one-variable function and visualise it by plotting it,\n",
    "\n",
    "$$ f(x) = y = x^2 - 4x + 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x):\n",
    "    return x**2 - 4*x + 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-7, 15, 100)\n",
    "Y = cost_function(X)\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(15.0, label='X')\n",
    "y = cost_function(x)\n",
    "\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know what is the gradient for the function:\n",
    "$$ \\frac{d(f(x))}{dx} = \\frac{dy}{dx} = 2x -4 $$ \n",
    "\n",
    "However, let's use a magic function of backprop where we do not explicitly calculate the gradient and let's see how it calculates the gradient. \n",
    "\n",
    "We know, that the gradient of the function should be 26 when $x$ = 15.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the gradient information to update $x$ and visualize the process to understand how $x$ changes over the iterations.\n",
    "\n",
    "Can we find the minima of the function by just **iteratively** doing this updation?\n",
    "\n",
    "We aim to find the minima of the function by iteratively updating \n",
    "\n",
    "$$ w^{(k+1)} = w^{(k)} - \\alpha \\nabla_w \\mathcal{J}(w^{(k)})$$\n",
    "\n",
    "Here, $\\alpha$ is step size for gradient descent also called learning rate. \n",
    "\n",
    "The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step. \n",
    "Choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network.\n",
    "\n",
    "Here's a more detailed explanation and visualization of how different learning rates affect gradient descent:\n",
    "\n",
    "- **Too Small Learning Rate**: The optimization process will be slow, requiring many iterations to converge to the minimum.\n",
    "- **Too Large Learning Rate**: The algorithm may overshoot the minimum, causing the optimization to diverge or bounce around the minimum without converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3 # 0.1 # 0.3 # 0.75 #\n",
    "num_iterations = 10\n",
    "\n",
    "x = Value(15.0, label='X')\n",
    "\n",
    "xy_list = []\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Calculate f(x)\n",
    "    y = cost_function(x)\n",
    "    \n",
    "    # Calculate dy/dx\n",
    "    y.backward()\n",
    "    \n",
    "    xy_list.append((x.data, y.data))\n",
    "    print(f\"Step: {i+1:2d} | X: {x.data:5.2f} | f(X): {y.data:8.4f} | Gradient dy/dx: {x.grad:7.4f}\")\n",
    "    \n",
    "    # Update x \n",
    "    x -= alpha * x.grad\n",
    "\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "\n",
    "xy_list = np.asarray(xy_list)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax  = fig.subplots()\n",
    "\n",
    "ax.plot(X, Y)\n",
    "ax.plot(xy_list[:, 0], xy_list[:, 1], 'r--', marker=\"o\")\n",
    "\n",
    "texts = []\n",
    "for i in range(len(xy_list)):\n",
    "    text = ax.text(xy_list[i, 0], xy_list[i, 1], f\"({i+1}, {round(xy_list[i, 0], 2)}, {round(xy_list[i, 1], 4)})\", ha='center', va='center')\n",
    "    texts.append(text)\n",
    "adjust_text(texts, expand=(3, 3.5), arrowprops=dict(arrowstyle='->', color='grey'))\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Cost Function', title=f\"$f(x) = y = x^2 - 4x + 3$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try some 2D function and see how it goes for the minima. \n",
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Can you guess how gradients will be calculated manually for multivariate functions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/classification.png\" style=\"height:1em\"> Moving towards Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completed a **regression task**, where we followed the function to locate its minima.\n",
    "\n",
    "But what approach should we take for **classification**?\n",
    "\n",
    "Furthermore, how do we handle an approximation of the function using **sampled points**?\n",
    "\n",
    "First, let's visualize what I'm referring to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = datasets.make_moons(n_samples=100, noise=0.1)\n",
    "X_test,  y_test  = datasets.make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "# make y be -1 or 1\n",
    "y_train = y_train*2 - 1 \n",
    "y_test  = y_test*2 - 1\n",
    " \n",
    "cmap = plt.cm.Spectral\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=[cmap(i%200) for i in y_train],               s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=[cmap(i%200) for i in y_test], c='w', s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous figure, we observe that we don't have a precisely defined function for either the \"left-moon\" or \"right-moon\". \n",
    "Furthermore, our objective is to separate these two moons rather than locate a minimum.\n",
    "\n",
    "<br>\n",
    "\n",
    "This is precisely the task where current deep learning networks, or artificial neural networks, excel the most!\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/ML-DL.mp4\" controls>\n",
    "</div>\n",
    "\n",
    "Deep learning is a subset of machine learning distinguished by its ability to automatically learn representations from data such as images, video, or text, without relying on human domain knowledge.\n",
    "\n",
    "Let's delve into neural networks, which form the foundation of deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/lab.png\" style=\"height:1em\"> Microscopic view of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/neuron.png\" style=\"height:1em\"> Visualising Artificial Neuron (Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Neuron.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "Feed-Forward Neural Networks are inspired by the information processing of one or more neural cells, known as neurons. \n",
    "These neurons are the fundamental computational units of the brain. \n",
    "A neuron receives input signals through its dendrites, which transmit the electrical signal to the cell body. \n",
    "The axon then carries the signal out to synapses, which are connections from one neuron's axon to another neuron's dendrites.\n",
    "\n",
    "**Did you know**: The human brain contains approximately **100 billion** neurons and is connected by approximately **10<sup>14</sup>-10<sup>15</sup>** synapses. \n",
    "Can you guess how many parameters state-of-the-art neural networks have?\n",
    "\n",
    "<details>\n",
    "<summary> Click to read more on Biological Neurons </summary>\n",
    "<br>\n",
    "\n",
    "The human nervous system consists of over 100 billion cells called neurons. \n",
    "Neurons are specialized cells in the nervous system that process and transmit information.\n",
    "\n",
    "Neurons have three main parts:\n",
    "\n",
    "- The cell body, or **soma**, which contains the nucleus and maintains the cell's function\n",
    "- Branching extensions called **dendrites**, which receive signals from other neurons and transmit them to the cell body\n",
    "- A long fiber called an **axon**, which transmits signals away from the cell body to other neurons, muscles, or glands\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://c4.staticflickr.com/3/2656/4253587827_9723c3ffd3_z.jpg\" />\n",
    "*Photo courtesy of GE Healthcare, http://www.flickr.com/photos/gehealthcare/4253587827/ *\n",
    "\n",
    "<img src=\"https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg\"/>\n",
    "</div>\n",
    "\n",
    "Some neurons have hundreds or thousands of dendrites, allowing them to receive input signals from numerous other neurons. \n",
    "The axons are also specialized; some, such as those that send messages from the spinal cord to the muscles in the hands or feet, may be very long---even up to several feet in length. \n",
    "To enhance signal transmission speed and prevent signal loss, axons are often surrounded by a fatty layer called the **myelin sheath**.\n",
    "The myelin sheath is a layer of fatty tissue surrounding the axon of a neuron that both acts as an insulator and allows faster transmission of the electrical signal.\n",
    "Axons branch out toward their ends, and at the tip of each branch is a *terminal button*.\n",
    "\n",
    "<br>\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary> History of Development of Perceptron </summary>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://news.cornell.edu/sites/default/files/styles/story_thumbnail_xlarge/public/2019-09/0925_rosenblatt_main.jpg\">\n",
    "    <br>\n",
    "    Frank Rosenblatt: Psychologist responsible for the first hardware implementation of the neural network (1957).\n",
    "    <br>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg\">\n",
    "    <br>\n",
    "    Mark I Perceptron machine, the first implementation of the perceptron algorithm designed by Frank Rosenblatt\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "**Simplified Model**. \n",
    "It's essential to note that the perceptron model of a biological neuron is highly simplified. \n",
    "For example, there are many different types of neurons, each with different properties. \n",
    "The dendrites in biological neurons perform complex nonlinear computations. \n",
    "The synapses are not just a single weight, theyâ€™re a complex non-linear dynamical system. \n",
    "The exact timing of the output spikes in many systems is known to be important, suggesting that the rate code approximation may not hold. \n",
    "Therefore, drawing direct analogies between artificial Neural Networks and real brains should be approached with caution. [[Brunel et.al.](https://www.sciencedirect.com/science/article/abs/pii/S0959438814000130)]\n",
    "\n",
    "In the context of Neural Networks, we abstract these complexities into three primary components:\n",
    "\n",
    "* **Activation:** Represents the neuron's output or level of activity\n",
    "* **Bias:** Represents a default or threshold value that shifts the neuron's activation\n",
    "* **Weight:** Represents the strength of the connection between neurons\n",
    "\n",
    "Additionally, there's a transfer function that computes the neuron's activation by combining the weighted inputs, adding the bias, and applying a nonlinear transformation to limit the output range.\n",
    "This limits the activations from growing too big or too small. (More on this later...)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Perceptron.png\"/>\n",
    "</div>\n",
    "\n",
    "A perceptron computes its activation value based on:\n",
    "a) incoming activations from connected neurons,\n",
    "b) weights associated with these connections, and\n",
    "c) a bias term.\n",
    "\n",
    "The net input is a weighted sum of all the incoming activations (linear combination) plus the neuron's bias value:\n",
    "\n",
    "$$ y = \\psi(x) =  f(b + \\sum\\limits_{i=1}^n x_i w_i)$$\n",
    "\n",
    "where $w_{i}$ is the weight, or connection strength, from the $i^{th}$ neuron, $x_i$ is the  input, $b$ is the bias value, and $f$ is the activation function applies a nonlinear transformation to the sum of weighted inputs.\n",
    "\n",
    "**TL;DR** In summary, a perceptron transforms input data by applying a nonlinear function to a weighted sum of inputs, capturing essential aspects of how neurons process information in biological systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# Weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# Bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical applications, individual perceptrons are frequently employed as foundational components for more sophisticated models, such as multi-layer perceptrons (MLPs), which are capable of addressing a broader array of challenges. \n",
    "The basic perceptron model is limited to classification tasks where the data is linearly separable.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Perceptron-Classification.png\" width=50%/>\n",
    "    <img src=\"./assets/images/Perceptron-Learning.gif\" width=45%/>\n",
    "</div>\n",
    "\n",
    "[[Minsky and Papert](https://leon.bottou.org/publications/pdf/perceptrons-2017.pdf)] showed that Perceptrons could not learn the XOR function.\n",
    "\n",
    "</div>\n",
    "    <img src=\"./assets/images/Perceptron-Problem.png\" width=50%/>\n",
    "    <img src=\"./assets/images/Perceptron-XOR.gif\" width=45%/>\n",
    "</div>\n",
    "\n",
    "Each activation function (or non-linearity) operates on a single number by performing a specific predefined mathematical operation. \n",
    "Various activation functions commonly encountered in practice include:\n",
    "\n",
    "**Sigmoid:** \n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "**Tanh:**\n",
    "$$ \\tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1} $$\n",
    "**ReLU:** \n",
    "$$ \\text{ReLU}(x) = \\begin{cases} 0 & \\text{if} ~ x \\leq 0 \\\\ 1 & \\text{otherwise} \\end{cases} $$\n",
    "**LeakyReLU:** \n",
    "$$ \\text{LeakyReLU}(x) = \\begin{cases} \\alpha x & \\text{if} ~ x \\leq 0; ~ \\text{where}~\\alpha~\\text{is small constant (e.g., 0.01)} \\\\ 1 & \\text{otherwise} \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, dy, title, ax):\n",
    "    ax.plot(x, y, linewidth=3, label=\"f(x)\", color=\"#69acc7\")\n",
    "    ax.plot(x, dy, linewidth=3, label=\"f'(x)\", color=\"#97c784\")\n",
    "    ax.set_title(f\"Curve for {title} with its derivative\")\n",
    "    ax.legend(loc='best')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "from scipy.special import erf\n",
    "\n",
    "fig = plt.figure(figsize=(4*5, 4*2))\n",
    "axes = fig.subplots(2, 5)\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "y  = x \n",
    "dy = np.ones_like(x)\n",
    "plot(x, y, dy, 'Linear', axes[0, 0])\n",
    "\n",
    "y  = 1/(1+np.exp(-x)) \n",
    "dy = y*(1-y)\n",
    "plot(x, y, dy, 'Sigmoid', axes[0, 1])\n",
    "\n",
    "y  = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)) \n",
    "dy = 1-(y**2)\n",
    "plot(x, y, dy, 'Tanh', axes[0, 2])\n",
    "\n",
    "y  = np.maximum(x, 0) \n",
    "dy = np.heaviside(x,1) \n",
    "plot(x, y, dy, 'ReLU', axes[0, 3])\n",
    "\n",
    "alpha = 0.1\n",
    "y  = np.where(x<0, alpha*x, x)\n",
    "dy = np.where(x<0, alpha,   1)\n",
    "plot(x, y, dy, 'LeakyReLU', axes[0, 4])\n",
    "\n",
    "y  = np.heaviside(x,1) \n",
    "dy = np.zeros_like(y)\n",
    "plot(x, y, dy, 'Step', axes[1, 0])\n",
    "\n",
    "y  = np.log(1+np.exp(x))\n",
    "dy = 1/(1+np.exp(-x))\n",
    "plot(x, y, dy, 'Softplus', axes[1, 1])\n",
    "\n",
    "alpha = 2\n",
    "y  = np.where(x<=0, alpha*(np.exp(x)-1), x)\n",
    "dy = np.where(x<=0, alpha*np.exp(x), 1)\n",
    "plot(x, y, dy, 'ELU', axes[1, 2])\n",
    "\n",
    "f = 1 + np.exp(-x)\n",
    "y  = x/f\n",
    "dy = (f + (x*np.exp(-x)))/(f**2)\n",
    "plot(x, y, dy, 'Swish', axes[1, 3])\n",
    "\n",
    "s = x / np.sqrt(2)\n",
    "erf_prime = lambda x: (2 / np.sqrt(np.pi)) * np.exp(-(x ** 2))\n",
    "y  = 0.5 * x * (1 + erf(s))\n",
    "dy = 0.5 + 0.5 * erf(s) + ((0.5 * x * erf_prime(s)) / np.sqrt(2))\n",
    "plot(x, y, dy, 'GELU', axes[1, 4])\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Quiz-Perceptron.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/deep-learning.png\" style=\"height:1em\"> Network of Neurons (Multi Layer Perceptron --- MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a network of neurons, we begin by organizing neurons into **layers**, forming acyclic graphs.\n",
    "\n",
    "A typical **Artificial Neural Network** consists of three primary layers: **input**, **hidden**, and **output**. \n",
    "Each layer comprises a collection of neurons, typically fully connected to the neurons in the subsequent layer. \n",
    "This means every input neuron connects with weighted connections to every hidden neuron, and similarly, every hidden neuron connects to every output neuron. \n",
    "This structure ensures that cycles are not present, preventing infinite loops during the network's forward pass.\n",
    "Instead of an amorphous blobs of connected neurons, Neural Network models are often organized into distinct layers of neurons. \n",
    "\n",
    "Neural Network models organize neurons into distinct layers rather than having them interconnected in an undifferentiated manner.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/ANN.png\" width=50%/>\n",
    "    <br>\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/NeuralNetwork.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "The network processes information as follows: \n",
    "input flows forward from the input layer through the hidden layers and ultimately through the output layer to produce a response. \n",
    "Each neuron within any layer uses the same activation function to propagate its information forward to the subsequent layer.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/ForwardPropagation.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "The output of the hidden layer, termed **features**, represents multiple layers of nonlinear transformations learned by the neural network (e.g., edges and shapes), which are then combined in the final layer to make predictions about more complex objects.\n",
    "\n",
    "This characteristic distinguishes neural networks as general nonlinear classifiers, contrasting with perceptrons, which are linear classifiers.\n",
    "\n",
    "<details>\n",
    "<summary> Neural Networks as Universal Approximators </summary>\n",
    "\n",
    "Neural Networks with fully-connected layers define a family of functions parameterized by their weights. \n",
    "It's pertinent to ask: \n",
    "What is the expressive power of this family of functions? \n",
    "Specifically, are there functions that cannot be modeled with a Neural Network?\n",
    "\n",
    "Interestingly, Neural Networks with at least one hidden layer are universal approximators [[Cybenko](https://link.springer.com/article/10.1007/BF02551274), [Nielsen](http://neuralnetworksanddeeplearning.com/chap4.html)]. \n",
    "This means that for any continuous function $g(x)$ and any small $ \\epsilon>0 $, there exists a Neural Network $\\psi(x)$ with one hidden layer  (using a suitable non-linearity, such as sigmoid) such that $âˆ£ f(x)âˆ’g(x) âˆ£ < \\epsilon$ for all $x$. \n",
    "In other words, neural networks can approximate any continuous function arbitrarily closely.\n",
    "\n",
    "If one hidden layer suffices to approximate any function, why use more layers and go deeper? \n",
    "The answer is that the fact that a two-layer Neural Network is a universal approximator, while intriguing mathematically, is not always practically useful. \n",
    "In one dimension, the ''sum of indicator bumps'' function $f(x)= \\sum_i c_i1(a_i<x<b_i)$ where a,b,c are parameter vectors is also a universal approximator, but no one would suggest that we use this functional form in Machine Learning. \n",
    "Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). \n",
    "\n",
    "Empirically, deeper networks (with multiple hidden layers) often perform better than shallow networks, despite having equivalent representational power.\n",
    "As an aside, in practice, three-layer neural networks often outperform two-layer networks, but deeper networks (four, five, or more layers) may not consistently offer additional benefits.\n",
    "\n",
    "</details>\n",
    "\n",
    "Furthermore, neural networks can have various topologies, such as; \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Architectures.png\"/>\n",
    "    <br>\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Quiz-NN.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "Next, let's proceed to create a neuron, a layer, and a network of neurons using our defined Value class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    # Explictly make gradients 0.0\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    # List of Parameters\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # Initialises weights, bias and activations for the neuron\n",
    "    def __init__(self, nin, activation='ReLU', layer_name='', neuron_name=''):\n",
    "        \n",
    "        self.w = [Value(np.random.uniform(-1,1), label=f\"Weight of {layer_name} {neuron_name} for Input {i+1}\") for i in range(nin)]\n",
    "        self.b = Value(0, label=f\"Bias of {layer_name} {neuron_name}\")\n",
    "        self.activation = activation\n",
    "\n",
    "    # Sets the list of parameters in the neuron\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    # Information when printing neuron\n",
    "    def __repr__(self):\n",
    "        return f\"{self.activation}Neuron(nin={len(self.w)})\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the neuron\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        w = sum((wi*xi for wi,xi in zip(self.w, x)))\n",
    "        out = w + self.b\n",
    "        \n",
    "        if self.activation == 'ReLU':\n",
    "            out = out.relu()\n",
    "        elif self.activation == 'Tanh':\n",
    "            out = out.tanh()\n",
    "        elif self.activation == 'Linear':\n",
    "            out = out\n",
    "            \n",
    "        return out\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        # Define neurons of a layer\n",
    "        self.neurons = [Neuron(nin, neuron_name=f\"Neuron {i+1}\", **kwargs) for i in range(nout)]\n",
    "\n",
    "    # Sets the list of parameters in the layer\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    # Information when printing layer\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [ {', '.join(str(n) for n in self.neurons)} ]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the layer\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts, activations=None):\n",
    "        if activations is not None:\n",
    "            assert len(nouts) == len(activations), 'Activations not defined for some layers'\n",
    "        else:\n",
    "            activations = ['Linear'] * len(nouts)\n",
    "            \n",
    "        sz = [nin] + nouts \n",
    "        \n",
    "        # Define layers of a MLP\n",
    "        self.layers = [Layer(sz[i], sz[i+1], activation=activations[i], layer_name=f\"Layer {i+1}\") for i in range(len(nouts))]\n",
    "\n",
    "    # Sets the list of parameters in the MLP\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    # Information when printing MLP\n",
    "    def __repr__(self):\n",
    "        new_line = f\"\\n{'-'*8}> \"\n",
    "        return f\"MLP of [{new_line}{new_line.join(str(layer) for layer in self.layers)}\\n]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the MLP\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function that will compute **loss** *(reward/penalty)* for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch_size=None, X=X_train, y=y_train):\n",
    "    \n",
    "    # Process Data in batches, in case data is too big to handle\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    \n",
    "    # Format Data to our Datatype\n",
    "    inputs = [ [Value(xrow[0], label='X'), Value(xrow[1], label='Y')] for xrow in Xb]\n",
    "    \n",
    "    # Forward Pass to get the scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Max-Margin Loss to calculate fitness based on scores and y\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    output_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # L2 Regularization (Optional)\n",
    "    ## To improve performance, we also regularise the parameters. \n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    \n",
    "    # Compute Final Loss -> Max-Margin Loss + L2 Regularization Loss\n",
    "    loss = output_loss + reg_loss\n",
    "    \n",
    "    # Compute Predictions and Accuracy\n",
    "    predictions = np.array([1 if (scorei.data > 0) else -1 for scorei in scores])\n",
    "    accuracy    = sum([(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)])/len(yb)\n",
    "    \n",
    "    # Return everything required\n",
    "    data = {}\n",
    "    data['loss']        = loss\n",
    "    data['scores']      = scores\n",
    "    data['predictions'] = predictions\n",
    "    data['accuracy']    = 100*accuracy\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(nin=2, nouts=[2, 2, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = compute_loss(model)\n",
    "print(f\"Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']: 5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/update.png\" style=\"height:1em\"> Backpropagation --- Trick to update weights (parameters) of multilayer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Backpropagation.mp4\" controls></video>\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary> Click Here to Read History on BackPropagation </summary>\n",
    "<br>\n",
    "\n",
    "For many years, it was unknown how to learn the weights in a multi-layered neural network. \n",
    "(Actually, the idea of using simulated evolution to search for the weights could have been used, but no one thought to do that.) \n",
    "\n",
    "In addition, as shown above, In late 1960s [[Minsky and Papert](https://leon.bottou.org/publications/pdf/perceptrons-2017.pdf)] showed that Perceptrons could not learn the XOR function. \n",
    "Hinting that you could not do simple functions without having multi-layers. \n",
    "\n",
    "This killed research into neural networks for more than a decade. \n",
    "So, the idea of neural networks generally was ignored until the mid 1980s when the **Back-Propagation of Error** (Backprop) was created [[Rumelhart et.al.](https://dl.acm.org/doi/10.5555/65669.104451)].\n",
    "</details>\n",
    "\n",
    "The **Backpropagation algorithm** (using *Backprop*), also called the *generalized delta rule*, is a *supervised* learning method for multilayer feed-forward networks in the field of Deep Learning.\n",
    "Technically, it is a method for training the weights in a multilayer feed-forward neural network. \n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected output signal. \n",
    "The system is trained using a *supervised learning* method, where the error between the systemâ€™s output and a known expected output is presented to the system and used to modify its internal state.\n",
    "\n",
    "With backpropagation, the errors are sent back through the network again and the weights are adjusted, improving the neural network. \n",
    "The neural network learns by varying the weights or parameters of a network so as to minimize the difference between the predictions of the neural network and the desired values. \n",
    "This process is repeated thousands of times, adjusting a neural network's weights in response to the error it produces, until the error can't be reduced anymore. \n",
    "This phase where the artificial neural network learns from the data is called training.\n",
    "During this process, the layers learn the optimal features for the model, which has the advantage that features do not need to be predetermined.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Backpropagation.png\"/>\n",
    "</div>\n",
    "\n",
    "Activation functions play a crucial role in these neural networks.\n",
    "\n",
    "> Additionally, there's a transfer function that computes the neuron's activation by combining the weighted inputs, adding the bias, and applying a nonlinear transformation to limit the output range.\n",
    "This limits the activations from growing too big or too small. (More on this later...)\n",
    "\n",
    "Let's understand how activation functions help in output range.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Exploding-Vanishing.png\"/>\n",
    "    <br>\n",
    "    <img src=\"./assets/images/ReLU.png\" width=75%/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scores'][0].backward()\n",
    "draw_dot(data['scores'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/practice.png\" style=\"height:1em\"> Train a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our neural network to classify the two moons. \n",
    "\n",
    "Backprop in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "n_log  = 1\n",
    "learning_rate = lr = 1.0\n",
    "\n",
    "model = MLP(nin=2, nouts=[16, 16, 1], activations=['ReLU', 'ReLU', 'Linear']) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "train_history = []\n",
    "\n",
    "# Optimize Iteratively\n",
    "for k in range(n_iter):\n",
    "    \n",
    "    # Zero-Grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward Pass -> Compute Loss\n",
    "    data = compute_loss(model)\n",
    "    \n",
    "    # Backward Pass\n",
    "    data['loss'].backward()\n",
    "        \n",
    "    # Log Details\n",
    "    if k % n_log == 0:\n",
    "        print(f\"Step: {k+1:3d} | Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']:5.2f}% | Learning Rate: {lr:.2f}\")\n",
    "        train_history.append((data['loss'].data, data['accuracy'], lr))\n",
    "    \n",
    "    # Update Weights using SGD\n",
    "    lr = learning_rate - 0.9*(k+1)/n_iter\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualise, Loss, Accuracy and Learning Rate over the iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6*3, 6))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "train_history = np.asarray(train_history)\n",
    "\n",
    "for ax, d, t in zip(axes.flat, [train_history[:, 0], train_history[:, 1], train_history[:, 2]], ['Loss', 'Accuracy', 'Learning Rate']):\n",
    "    ax.plot(d)\n",
    "    ax.set(title=t, xlim=(0, n_iter))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the predictions of the neural network and see which points are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = compute_loss(model, X=X_train, y=y_train)\n",
    "test_data  = compute_loss(model, X=X_test,  y=y_test)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the **boundary** of the trained neural network, how does the boundary separating the two moons look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Decision Boundary\n",
    "resolution = 0.25\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 \n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max+ resolution, resolution), np.arange(y_min, y_max+ resolution, resolution))\n",
    "\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "\n",
    "scores = list(map(model, inputs))\n",
    "\n",
    "Z = np.array([s.data > 0 for s in scores]).reshape(xx.shape)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.contourf(xx, yy, Z, colors=[cmap(-1%200), cmap(1)], alpha=0.25)\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(x_min, x_max), ylim=(y_min, y_max))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try some other simple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Multi-Layer Perceptron (MLPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "<summary> History of Understanding Visual Perception </summary>\n",
    "<br>\n",
    "[Hubel and Wiesel](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/)\n",
    "[Roberts](https://dspace.mit.edu/handle/1721.1/11589)\n",
    "[Marr](https://royalsocietypublishing.org/doi/pdf/10.1098/rspb.1970.0040)\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "### Convolution Neural Networks (CNNs) - Inspired by Visual Cortex\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=30%/>\n",
    "    <br>\n",
    "    Processing a group of spatially close inputs.\n",
    "    <br>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1358/1*D6iRfzDkz-sEzyjYoVZ73w.gif\"width=50%>\n",
    "    <br>\n",
    "    Kernel acts as filteration.\n",
    "</div>\n",
    "\n",
    "### Recurrent Neural Networks (RNNs) - Inspired by Memory\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif\" width=45%/>\n",
    "    <br>\n",
    "    Passing hidden state to next time step\n",
    "    <br>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*WMnFSJHzOloFlJHU6fVN-g.gif\"width=45%>\n",
    "    <br>\n",
    "    Processing inside a RNN Cell\n",
    "</div>\n",
    "\n",
    "### Gated Recurrent Unit (GRUs) - Improvement on RNNs for Long Term Memory\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*jhi5uOm9PvZfmxvfaCektw.png\" width=45%/>\n",
    "    <br>\n",
    "    GRU Cell\n",
    "</div>\n",
    "\n",
    "### Long Short Term Memory (LSTMs) - Improvement on RNNs for Long Term Memory\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*0f8r3Vd-i4ueYND1CUrhMA.png\" width=45%/>\n",
    "    <br>\n",
    "    LSTM Cell\n",
    "</div>\n",
    "\n",
    "### Transformers (Self-Attention) - Inspired by Attention\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./assets/images/Self-Attention.png\">\n",
    "    <br>\n",
    "    Self-Attention Processing in Transformers\n",
    "    <br>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*G8thyDVqeD8WHim_QzjvFg.gif\">\n",
    "    <br>\n",
    "    Exemplar Working of Self-Attention\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/action.png\" style=\"height:1em\"> Moving Forward: Example of Handwritten Digits Classification using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a task, how does one train a neural network to do/solve the task? This involves the following steps:\n",
    "\n",
    "1. Determine an appropriate network architecture.\n",
    "2. Define a data set that will be used for training.\n",
    "3. Define the neural network parameters to be used for training.\n",
    "4. Train the network.\n",
    "5. Test the trained network.\n",
    "6. Do post training analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/setup.png\" style=\"height:1em\"> Libraries Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/warning.png\" style=\"height:1em\"> Preferably run on [Google Collab](https://colab.research.google.com/) or a PC with Nvidia GPU  <img src=\"./assets/icons/warning.png\" style=\"height:1em\">\n",
    "\n",
    "We again start by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import repeat\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn, optim as optim\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = 'cpu' # 'cpu' # 'cuda' ## Use 'cuda' if you have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/construction.png\" style=\"height:1em\"> Determining an appropriate architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a neural network consists of an input layer, an output layer, and zero or more hidden layers. \n",
    "Once a network has been trained, when you present an input to the network, the network will propagate the inputs through its layers to produce an output. \n",
    "\n",
    "If the input represents an instance of the task, the output should be the solution to that instance after the network has been trained. Thus, one can view a neural network as a general pattern associator. \n",
    "In general, as far as the design of a neural network is concerned, you always begin by identifying the size of the input and output layers. \n",
    "\n",
    "Given a task, the first step is to identify the nature of inputs to the pattern associator. \n",
    "This is normally in the form of number of neurons required to represent the input. \n",
    "Similarly, you will need to determine how many output neurons will be required. \n",
    "\n",
    "First, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. \n",
    "That is, the space of representable functions grows since the neurons can collaborate to express many different functions. \n",
    "\n",
    "We can observe that Neural Networks with more neurons can express more complicated functions. \n",
    "However, this is both a blessing (since we can learn to classify more complicated data) and a curse (since it is easier to overfit the training data). \n",
    "**Overfitting** occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship. \n",
    "For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint decision regions. \n",
    "\n",
    "Based on our discussion above, it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting. \n",
    "However, this is incorrect - there are many other preferred ways to prevent overfitting in Neural Networks (such as L2 regularization, dropout, input noise). \n",
    "In practice, it is always better to use these methods to control overfitting instead of the number of neurons.\n",
    "\n",
    "The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: \n",
    "Itâ€™s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). \n",
    "Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. \n",
    "Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, \n",
    "e.g. in a recent paper The Loss Surfaces of Multilayer Networks. \n",
    "In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. \n",
    "On the other hand, if you train a large network youâ€™ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. \n",
    "In other words, all solutions are about equally as good, and rely less on the luck of random initialization.\n",
    "Through much empirical practice, you will develop your own heuristics about this. \n",
    "\n",
    "For the MNIST Dataset, we have used 4 MLP Layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/mining.png\" style=\"height:1em\"> Define a data set that will be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have decided on the network architecture, you have to prepare the data set that will be used for training. Each item in the data set represents an input pattern and the correct output pattern that should be produced by the network (since this is supervised training). \n",
    "\n",
    "In most tasks, there can be an infinite number of such input-output associations. Obviously it would be impossible to enumerate all associations for all tasks (and it would make little sense to even try to do this!). \n",
    "You have to then decide what comprises a good representative data set that, when used in training a network, would generalize to all situations.\n",
    "\n",
    "In the example of the AND, the data set is very small, finite (only 4 cases!), and **exhaustive**.\n",
    "\n",
    "However, MNIST Dataset consists of images of size 28*28 pixels which are monochromatic. Since monochromatic means either black or white, there exists $ 2^{28 \\times 28} $ possible images (this number is 237 digits long). If we go for greyscale, there would exist $ 256^{28\\times28} $ possible images (this number is 1889 digits long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 128\n",
    "test_batch_size = 512\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs  = {'batch_size': test_batch_size}\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "            \n",
    "train_dataset = datasets.MNIST('../data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test_dataset  = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader  = infinite_loader(torch.utils.data.DataLoader(train_dataset,**train_kwargs))\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(vutils.make_grid(batch[0].to(device)[:32], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/adjust.png\" style=\"height:1em\"> Define the neural network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the hyperparameters required to train the neural network.\n",
    "The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. \n",
    "If you train the neural network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. \n",
    "That is, the loss on the validation set will start increasing as the training set loss drops.\n",
    "\n",
    "#### Choose the number of iterations\n",
    "This is the number of batches of samples from the training data we'll use to train the network. The more iterations you use, the better the model will fit the data. However, if you use too many iterations, then the model with not generalize well to other data, this is called overfitting. You want to find a number here where the network has a low training loss, and the validation loss is at a minimum. As you start overfitting, you'll see the training loss continue to decrease while the validation loss starts to increase.\n",
    "\n",
    "#### Choose the learning rate\n",
    "This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. A good choice to start at is 0.1. If the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.\n",
    "\n",
    "#### Choose the number of hidden neurons\n",
    "The more hidden neurons you have, the more accurate predictions the model will make. Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden neurons is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden neurons you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter          = 1000\n",
    "learning_rate   = 0.1\n",
    "n_log           = 1\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0.01, T_max=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/predictive.png\" style=\"height:1em\"> Training and Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once all the parameters are specified, you start the training process. \n",
    "This involves presenting each input pattern to the network, propagating it all the way until an output is produced, comparing the output with the desired target, computing the error, backpropagating the error, and applying the learning rule. \n",
    "This process is repeated as needed. \n",
    "\n",
    "In general, you should train the network for several iterations, it can be anywhere from a few hundred to millions! depennding on the dataset. \n",
    "Gradually, the network will begin to show improved and stable performance. \n",
    "Performance of the network is measured according to the task, for our classificiation task, we measure the accuracy, while for regression task, we can measure mean squared error. \n",
    "\n",
    "You can either stop the training process after a certain number of iterations have elapsed, or after the performance has saturated (early-stopping).\n",
    "\n",
    "Once the network has been trained, it is time to test it. \n",
    "There are several ways of doing this. \n",
    "Perhaps the easiest is to turn learning off and then see the outputs produced by the network for each input in the data set. \n",
    "When a trained network is going to be used in a *deployed* application, all you have to do is save the weights of all interconnections in the network into a file. \n",
    "The trained network can then be recreated at anytime by reloading the weights.\n",
    "\n",
    "**Note**: Instead of training-then-testing, there is another methodology: you can test-while-training, which encompases other evaluation technqiues like cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, n_iter):\n",
    "    train_history = []\n",
    "    model.train()\n",
    "    with tqdm(total=n_iter) as bar:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Explictily Zeroing Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % n_log == 0:\n",
    "                bar.update(n_log)\n",
    "                bar.set_postfix({'Loss':  f\"{loss.item():.4f}\", 'Learning Rate': f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
    "                train_history.append((loss.item, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            # Changing Learning Rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx == n_iter-1:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    \n",
    "    images    = []\n",
    "    labels    = []\n",
    "    outputs   = []\n",
    "    \n",
    "    with torch.no_grad() and tqdm(total=len(test_loader)) as bar:\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target, reduction='sum').item()  \n",
    "            \n",
    "            # Get Prediction\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            correct   += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss += loss\n",
    "            outputs.extend(output.detach().cpu().numpy())\n",
    "            images.extend(data.detach().cpu().numpy())\n",
    "            labels.extend(target.detach().cpu().numpy())\n",
    "            \n",
    "            bar.update(1)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct   /= len(test_loader.dataset)\n",
    "    images      = np.asarray(images).reshape(-1, 1, 28, 28).transpose(0, 2, 3, 1)\n",
    "    labels      = np.asarray(labels)\n",
    "    outputs     = np.asarray(outputs)\n",
    "\n",
    "    print(f\"Test set--- Average loss: {test_loss:.4f}, Accuracy: {100. * correct:.2f}%\")\n",
    "    return images, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, optimizer, scheduler, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model for reusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(model).save('mnist-try.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the saved model and evaluate it on our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.jit.load('mnist-try.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, outputs = test(trained_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/analytics.png\" style=\"height:1em\"> Do post training analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Perhaps the most important step in using neural networks is the analysis one performs once a network has been trained. There are a whole host of analysis techniques, here we present some of them which can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(embedding, images, labels, title):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    ax  = fig.subplots()\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(embedding)\n",
    "    \n",
    "    for digit in range(10):\n",
    "        ax.scatter(*X[labels == digit].T, marker=f\"${digit}$\", s=60, color=plt.cm.Dark2(digit), alpha=0.425, zorder=2,)\n",
    "    \n",
    "    shown_images = np.array([[1.0, 1.0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(images[i], cmap=plt.cm.gray_r), X[i])\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "num_images = 1000\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2)\n",
    "\n",
    "image_embedding = tsne.fit_transform(images[:num_images].reshape(num_images, -1), labels[:num_images])\n",
    "plot_embedding(image_embedding, images[:num_images], labels[:num_images], \"Raw Data Representation\")\n",
    "\n",
    "output_embedding = tsne.fit_transform(outputs[:num_images].reshape(num_images, -1), labels[:num_images])\n",
    "plot_embedding(output_embedding, images[:num_images], labels[:num_images], \"Neural Network Representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try more advanced adaptive optimizers like Adam \n",
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Try some more innovative analysis to understand the trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/jester.png\" style=\"height:1em\"> Bonus: Fool the trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/setup.png\" style=\"height:1em\"> Libraries Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Install git from [this website](https://git-scm.com/download/win). Alternatively, if you have Anaconda or Miniconda installed you can use `conda install git`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/RobustBench/robustbench.git \n",
    "\n",
    "from scipy.optimize import differential_evolution as de\n",
    "\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/data-modeling.png\" style=\"height:1em\"> Loading Data and Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = load_cifar10(n_examples=50)\n",
    "model = load_model(model_name='Standard', dataset='cifar10', threat_model='Linf').to(device)\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 10\n",
    "image = x_test[image_id]\n",
    "label = y_test[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.softmax(model(image[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/anomaly.png\" style=\"height:1em\"> Defining Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image(perturbations, img):\n",
    "    \n",
    "    perturbations = perturbations.astype(int)\n",
    "    if perturbations.ndim < 2:\n",
    "        perturbations = np.array([perturbations])\n",
    "    \n",
    "    img = (img * 255).detach().cpu().numpy().astype(int)\n",
    "    imgs = np.tile(img, [len(perturbations)] + [1]*(perturbations.ndim+1))\n",
    "    \n",
    "    for x, img in zip(perturbations, imgs):\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            x_pos, y_pos, *value = pixel\n",
    "            img[:, round(x_pos), round(y_pos)] = value\n",
    "        \n",
    "    imgs = imgs/255.0\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = np.array([6, 6, 255, 0, 0]) \n",
    "image_perturbed = perturb_image(pixel, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"./assets/icons/cyber-attack.png\" style=\"height:1em\"> Few-Pixel Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(image, target, pixel_count=1, maxiter=100, popsize=256):\n",
    "    \n",
    "    bounds = [(0,32), (0,32), (0,256), (0,256), (0,256)] * pixel_count\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "    \n",
    "    def run(perturbations, image, target, evaluate=False):\n",
    "        images_perturbed = perturb_image(perturbations, image).to(device)\n",
    "        probabilities = torch.softmax(model(images_perturbed), dim=1)\n",
    "        if evaluate:\n",
    "            prediction = torch.argmax(probabilities, dim=1)\n",
    "            return prediction != target\n",
    "        else:\n",
    "            confidence = probabilities[:, target].detach().cpu().numpy()\n",
    "            return confidence\n",
    "    \n",
    "    def predict_fn(xs):\n",
    "        xs = xs.transpose()\n",
    "        return run(xs, image, target, evaluate=False)\n",
    "    \n",
    "    def callback_fn(x, convergence):\n",
    "        return run(x, image, target, evaluate=True)\n",
    "\n",
    "    result = de(\n",
    "        predict_fn, bounds=bounds, maxiter=maxiter, popsize=popmul, vectorized=True,\n",
    "        recombination=1, atol=-1, callback=callback_fn, polish=False, disp=True)\n",
    "    \n",
    "    return result.x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_perturbation = attack(image, label, pixel_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_perturbed = perturb_image(attacked_perturbation, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "true_conf = output[:, label]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\\nTrue Class {classes[label]} has {true_conf.item()*100:.2f}% condifence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Instead of DE, try to use SGD to optimize the attack\n",
    "#### <img src=\"./assets/icons/try.png\" style=\"height:1em\"> **Try-it-out**: Can you modify to create a targeted attack, where you can define what would be the predicted class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/owl.png\" style=\"height:1em\"> Words of Wisdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/videos/Limitations.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assists in achieving better fit and in learning complex patterns. \n",
    "Bias allows the perceptron to make adjustments to its output independently of the inputs. \n",
    "It acts like the intercept added in a linear equation, which helps the model in a way that can best fit for the given data. \n",
    "Bias also allows the network to learn and represent more complex relationships between the input and output variables.\n",
    "\n",
    "- Handling zero inputs and mitigating the problems of Vanishing Gradients. \n",
    "Activation functions introduce non-linearity into the perceptron. \n",
    "Bias helps the perceptron shift the activation function towards positive or negative side. \n",
    "By adding the bias, perceptron can train over points that do not pass through origin, thereby ensuring that a neuron can activate even when all of its input values are zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid. \n",
    "It takes a real-valued number and ''squashes'' it into range between 0 and 1. \n",
    "In particular, large negative numbers become 0 and large positive numbers become 1. \n",
    "The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: \n",
    "from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). \n",
    "In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. \n",
    "It has two major drawbacks:\n",
    "\n",
    "    - Sigmoids saturate and kill gradients. \n",
    "    A very undesirable property of the sigmoid neuron is that when the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. \n",
    "    Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate's output for the whole objective. \n",
    "    Therefore, if the local gradient is very small, it will effectively ''kill'' the gradient and almost no signal will flow through the neuron to its weights and recursively to its data.\n",
    "    Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. \n",
    "    For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.\n",
    "\n",
    "    - Sigmoid outputs are not zero-centered. \n",
    "    This is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. \n",
    "    This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive, then the gradient on the weights $w$ will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$. \n",
    "    This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. \n",
    "    However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. \n",
    "    Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.\n",
    "\n",
    "- Tanh. \n",
    "It squashes a real-valued number to the range [-1, 1]. \n",
    "Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. \n",
    "Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. \n",
    "Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\\tanh(x)=2\\sigma(2x)âˆ’1$\n",
    "\n",
    "- ReLU.\n",
    "The Rectified Linear Unit has become very popular in the last few years. \n",
    "It computes the function $f(x)=\\max(0,x)$. \n",
    "In other words, the activation is simply thresholded at zero. \n",
    "There are several pros and cons to using the ReLUs:\n",
    "    \n",
    "    - (+) It was found to greatly accelerate (e.g. a factor of 6 in [[Krizhevsky et al.](https://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)]) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. \n",
    "    It is argued that this is due to its linear, non-saturating form.\n",
    "\n",
    "    - (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.\n",
    "\n",
    "    - (-) Unfortunately, ReLU units can be fragile during training and can ''die''. \n",
    "    For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. \n",
    "    If this happens, then the gradient flowing through the unit will forever be zero from that point on. \n",
    "    That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. \n",
    "    For example, you may find that as much as 40% of your network can be ''dead'' (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. \n",
    "    With a proper setting of the learning rate this is less frequently an issue.\n",
    "\n",
    "- LeakyReLU.\n",
    "Leaky ReLUs are one attempt to fix the ''dying ReLU'' problem.\n",
    "Instead of the function being zero when x < 0, a leaky ReLU will instead have a small positive slope (of 0.01, or so). \n",
    "Some people report success with this form of activation function, but the results are not always consistent. \n",
    "The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in [He et al.](https://arxiv.org/abs/1502.01852). \n",
    "However, the consistency of the benefit across tasks is presently unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on Gradients and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numerical vs Analytic Gradient.\n",
    "There are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). \n",
    "\n",
    "- Practical considerations while using Numerical Gradient. \n",
    "Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5). \n",
    "Ideally, you want to use the smallest step size that does not lead to numerical issues. \n",
    "Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula: $\\frac{f(x+h)âˆ’f(xâˆ’h)}{2h}$. [[Wiki- Numerical Differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation)]\n",
    "\n",
    "- Local search algorithm. \n",
    "So in general it can get stuck in local optima. \n",
    "We use examples from linear regression which is a convex optimization problem, so for a small enough learning rate, it will converge to a global optimum.\n",
    "\n",
    "- Fixed Points of Gradient Descent, i.e., values of $ w^{(k+1)} = w^{(k)} $.\n",
    "These are the stationary points where differentiation is 0, i.e., $\\nabla \\mathcal(J) = 0$. \n",
    "They maybe stable (e.g. local optima) ot unstable (e.g. saddle points).\n",
    "\n",
    "- Invariances of Gradient Descent. \n",
    "It is invariant to rigid transformations (rotation, reflection, and translation)\n",
    "\n",
    "- Batched Stocastic Gradient Descent. \n",
    "It is possible to run stochastic gradient descent where every step we touch more than one example.\n",
    "This is called minibatch stochastic gradient descent and the number of examples we look at per iterations is called the ''minibatch size''. \n",
    "(When the minibatch size is 1, we recover stochastic gradient descent.) \n",
    "In many environments, using a larger minibatch can be a good idea because the gradient is less noisy (computed as an average over the examples) and faster because matrix-vector libraries\n",
    "work better with larger matrices.\n",
    "\n",
    "- Unbiased Estimate. Stocastic Gradient Descent makes significant progress before it has even looked at all the data, this is because if we sample a training example at random SGD gives an unbiased estimate of the batched gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalisation (or Standardisation). \n",
    "A coomon trick when training machine learning models is to normalize or standardize the inputs to zero mean and unit variance. This ensures faster convergenve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Neural Network\n",
    "\n",
    "- Random Intialisation of Weights. \n",
    "It's important to ''**break the symmetry**'' of the neurons in the networks or, in other words, force the neurons to be different at the beginning. \n",
    "This means that itâ€™s important to *initialize* the parameters w and b randomly. \n",
    "A good method for random initialization is Gaussian random or uniform random.\n",
    "Sometimes tuning the variance of the initialization also helps.\n",
    "Also make sure that the random initialization does not ''**saturate**'' the networks. \n",
    "This means that most of the time, for your data, the values of the neurons should be between 0.2 and 0.8. \n",
    "This is because we do not want to neurons to have too many values of zeros and ones. \n",
    "When that happens, the gradient is small and thus the training is much longer.\n",
    "\n",
    "- Choosing Learning Rate.\n",
    "Picking a good learning rate Î± can be tricky. \n",
    "A large learning rate can change the parameters too aggressively or a small learning rate can change the parameters too conservatively. \n",
    "Both should be avoided, a good learning rate is one that leads to good overall improvements in the objective function. \n",
    "To select good $\\alpha$, itâ€™s also best to monitor the progress of your training. \n",
    "In many cases, a learning rate of 0.1 or 0.01 is a very good start, if you use SGD.\n",
    "For other optimizers like Adam, starting learning rate can differ. \n",
    "\n",
    "- Choosing Hyperparamters. \n",
    "Picking good hyperparameters (architectural parameters such as number of layers, number of neurons\n",
    "on each layers) for your neural networks can be difficult and is a topic of current research. \n",
    "A standard way to pick architectural parameters is via cross-validation: Keep a hold-out validation set that the training never touches. \n",
    "If the method performs well on the training data but not the validation set, then the model overfits: it has too many degrees of freedom and remembers the training cases but does not generalize to new cases. \n",
    "If the model overfits, we need to reduce the number of hidden layers or number of neurons on each hidden layer. \n",
    "If the method performs badly on the training set then the model underfits: it does not have enough degrees of freedom and we should increase the number of hidden layers or number of neurons. \n",
    "Be warned that bad performance on the training set can also mean that the learning rate is chosen poorly.\n",
    "\n",
    "- Hyperparameter Optimization. \n",
    "Picking good hyperparameters can also be automated using grid search, random search or Bayesian\n",
    "optimization. \n",
    "In grid search, every possible combination of hyperparameters will be tried and crossvalidated with a hold-out validation set. \n",
    "In case that grid search is expensive because the number of hyperparameters is large, one can try random search where hyperparameter configurations are generated and tried at random. \n",
    "Bayesian optimization looks at the performances of networks at previous hyperparameter combinations and fits a function through these points, it then picks the next combination that maximizes some utility function such as the mean plus some function of the uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory footprint\n",
    "\n",
    "- Precision of float variables. \n",
    "It is possible to use single precision (float16) for the parameters in the networks instead of double precision (float32). \n",
    "This reduces the memory footprint of the model in half and usually does not hurt the performances of the networks. \n",
    "A downside is that it is more tricky to check the correctness of the gradient using the numerical approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Experience\n",
    "\n",
    "- Monitoring. \n",
    "Have a way to monitor the progress of your training. \n",
    "Perhaps the best method is to compute the objective function on the current example or on a subset of the training data or on a held-out validation set. \n",
    "\n",
    "- Code Optimization. \n",
    "Neural networks can take a long time to train and thus itâ€™s worth spending time to optimize the code\n",
    "for speed. \n",
    "To speed up the training, make use of fast matrix vector libraries, which often provide good speed-up over na ÌˆÄ±ve implementation of matrix vector multiplication. The vectorized version of the backpropagation algorithm will come in handy in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/thank-you.png\" style=\"height:1em\"> Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [SimpliLearn - What is Artificial Network?](https://youtu.be/uMzUB89uSxU?si=KyY5C5HhoDnDlAeL)\n",
    "- [SimpliLearn - What is a Neural Network?](https://www.youtube.com/watch?v=bfmFfD2RIcg)\n",
    "- [SimpliLearn - What is Deep Learning?](https://www.youtube.com/watch?v=6M5VXKLf4D4)\n",
    "- [Andrej Karpathy - Micrograd Library](https://github.com/karpathy/micrograd)\n",
    "- [Fei-Fei Li - Standford University CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/index.html)\n",
    "- [Flaticon - Icons](https://www.flaticon.com/free-icons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"./assets/icons/books.png\" style=\"height:1em\"> Extra Study Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Increasing order of complexity)\n",
    "\n",
    "- [3Blue1Brown - DeepLearning](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [Nvidia - DeepLearning Blog](https://www.nvidia.com/en-us/glossary/deep-learning/)\n",
    "- [James Loy - How to build your own Neural Network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)\n",
    "- [Douglas Blank - Artificial Neural Networks](https://jupyter.brynmawr.edu/services/public/dblank/BioCS115%20Computing%20through%20Biology/2016-Spring/Notebooks/Artificial_Neural_Networks.ipynb)\n",
    "- [Quoc V. Le - A tutorial on Deep Learning](https://cs.stanford.edu/~quocle/tutorial1.pdf)\n",
    "- [Matt Mazur - A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
    "- [Michael Nielsen - Neural Networks and Deep Learning ](http://neuralnetworksanddeeplearning.com)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Lecture 1](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/slides/lec01.pdf)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Tutorial 1](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/tutorials/tut01.pdf)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Lecture 7](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/slides/lec07.pdf)\n",
    "- [Roger Grosse - University of Toronto CSC2541 Winter 2022 Lecture 9](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/slides/lec09.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
