{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install graphviz\n",
    "%pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from graphviz import Digraph\n",
    "from adjustText import adjust_text\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a grid of Xs and Ys\n",
    "resolution = 100\n",
    "X, Y = np.meshgrid( np.linspace(-1,1,resolution), np.linspace(-1,1,resolution) )\n",
    "\n",
    "# Defining 4 different 2D functions\n",
    "mux, muy, sigma = 0.3, -0.3, 4\n",
    "G1 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = -0.3, 0.3, 2\n",
    "G2 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = 0.6, 0.6, 2\n",
    "G3 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux ,muy, sigma = -0.4, -0.2, 3\n",
    "G4 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "\n",
    "# Composing the final function\n",
    "G = G1 + G2 - G3 - G4\n",
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(6*4,6)) # Defining the figure space\n",
    "axes = fig.subplots(1, 4)          # Defining the subplots in the figure\n",
    "\n",
    "for ax, g, t in zip(axes.flat, [G1, G2, G3, G4], ['G1', 'G2', 'G3', 'G4']): # Iterating over axes and functions\n",
    "    ax.imshow(g, vmin=-1, vmax=1, cmap='jet')                               # Ploting the function on the subplot\n",
    "    ax.set(title=t, xlim=(0, 100), ylim=(0, 100))                           # Setting the title and limits of the subplot\n",
    "\n",
    "fig.tight_layout() # Removes extra spacing from the figure\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cax = ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "ax.set(title=\"Function\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.colorbar(cax) # Attaching the colorbar to the figure\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()               # Instruct Matplotlib to show the figures created\n",
    "\n",
    "# fig.savefig(\"./assets/images/Optimization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5     # Number of Steps to take for optimisation\n",
    "alpha  = 0.03  # Learning rate of the optimisation\n",
    "\n",
    "w = np.array([70.0, 60.0]) # Starting Parameter (Point)\n",
    "sigma  = 3                 # Standard deviation of the samples around current parameter vector\n",
    "\n",
    "fig  = plt.figure( figsize=(5*n_iter, 5) )\n",
    "axes = fig.subplots(1, n_iter) \n",
    "\n",
    "prevx, prevy = [], []\n",
    "for q, ax in zip(range(n_iter), axes):\n",
    "    \n",
    "    # Draw the Optimization Landscape\n",
    "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "\n",
    "    # Sample Random Population\n",
    "    noise = np.random.randn(200, 2)\n",
    "    wp = np.expand_dims(w, 0) + sigma * noise\n",
    "    x,y = zip(*wp)\n",
    "    \n",
    "    # Estimate Gradient (Direction)\n",
    "    R  = np.array([G[int(wi[1]), int(wi[0])] for wi in wp])\n",
    "    R -= R.mean()\n",
    "    R /= R.std() \n",
    "    g  = np.dot(R, noise)\n",
    "    u  = alpha * g\n",
    "    \n",
    "    prevx.append(w[0])\n",
    "    prevy.append(w[1])\n",
    "    \n",
    "    # Draw Population on Landscape (Black Points)\n",
    "    ax.scatter(x, y, 4, 'k', edgecolors='face')\n",
    "    \n",
    "    # Draw estimated gradient (direction) as arrow (White Arrow)\n",
    "    ax.arrow(w[0], w[1], u[0], u[1], head_width=3, head_length=5, fc='w', ec='w')\n",
    "    \n",
    "    # Draw Parameter History (White Points)\n",
    "    ax.plot(prevx, prevy, 'wo-')\n",
    "    \n",
    "    # Update Parameter According to the gradient\n",
    "    w += alpha * g\n",
    "    \n",
    "    ax.set(title=f\"Iteration: {q+1} | Reward: {G[int(w[0]), int(w[1])]:.2f}\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, label='', _children=(), _op=''):\n",
    "        \n",
    "        # Information about value, gradient and its name\n",
    "        self.data  = data\n",
    "        self.grad  = 0.0\n",
    "        self.label = label\n",
    "        \n",
    "        # Utility attributes for the calculating and passing gradients (Backprop)\n",
    "        self._backward = lambda: None\n",
    "        self._prev     = set(_children)\n",
    "        self._op       = _op \n",
    "    \n",
    "    # Simple arithemtic operations on value and computing corresponding gradients   \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, label='+', _children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, label='*', _children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad += self.data  * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, label=f'**{other}', _children=(self,), _op='**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Other arithmetic operations\n",
    "    ### Don't need to define backward functions since, they use __mul__ or __add__ for which backward is already defined. \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    # Simple transformations on Value and computing corresponding gradients\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, label='ReLU', _children=(self,), _op='ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        out = Value(t, label='Tanh', _children=(self, ), _op='Tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "  \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(np.exp(x), label='Exp',  _children=(self, ), _op='Exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Information when printing instance\n",
    "    def __repr__(self):\n",
    "        if self.label:\n",
    "            return f\"Value(node={self.label}, data={self.data}, grad={self.grad})\"\n",
    "        else:\n",
    "            return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # Recurisvely call backward -> Backprop\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the graph from a root node\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "# Visualizes the graph built from root node\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x):\n",
    "    return x**2 - 4*x + 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-7, 15, 100)\n",
    "Y = cost_function(X)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax  = fig.subplots()\n",
    "\n",
    "ax.plot(X, Y)\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Cost Function', title=f\"$f(x) = y = x^2 - 4x + 3$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(15.0, label='X')\n",
    "y = cost_function(x)\n",
    "\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # 0.1 # 0.3 # 0.9 #\n",
    "num_iterations = 10\n",
    "\n",
    "x = Value(15.0, label='X')\n",
    "\n",
    "xy_list = []\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Calculate f(x)\n",
    "    y = cost_function(x)\n",
    "    \n",
    "    # Calculate dy/dx\n",
    "    y.backward()\n",
    "    \n",
    "    xy_list.append((x.data, y.data))\n",
    "    print(f\"Step: {i+1:2d} | X: {x.data:5.2f} | f(X): {y.data:8.4f} | Gradient dy/dx: {x.grad:7.4f}\")\n",
    "    \n",
    "    # Update x \n",
    "    x -= alpha * x.grad\n",
    "\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "\n",
    "xy_list = np.asarray(xy_list)\n",
    "\n",
    "ax.plot(xy_list[:, 0], xy_list[:, 1], 'r--', marker=\"o\")\n",
    "\n",
    "texts = []\n",
    "for i in range(len(xy_list)):\n",
    "    text = ax.text(*xy_list[i], f\"({i+1}, {round(xy_list[i, 0], 2)}, {round(xy_list[i, 1], 4)})\", ha='center', va='center')\n",
    "    texts.append(text)\n",
    "adjust_text(texts, expand=(3, 3.5), arrowprops=dict(arrowstyle='->', color='grey'), ax=ax)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = datasets.make_moons(n_samples=100, noise=0.1)\n",
    "X_test,  y_test  = datasets.make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "# make y be -1 or 1\n",
    "y_train = y_train*2 - 1 \n",
    "y_test  = y_test*2 - 1\n",
    " \n",
    "cmap = plt.cm.Spectral\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=[cmap(i%200) for i in y_train],               s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=[cmap(i%200) for i in y_test], c='w', s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# Weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# Bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, dy, title, ax):\n",
    "    ax.plot(x, y, linewidth=3, label=\"f(x)\", color=\"#69acc7\")\n",
    "    ax.plot(x, dy, linewidth=3, label=\"f'(x)\", color=\"#97c784\")\n",
    "    ax.set_title(f\"Curve for {title} with its derivative\")\n",
    "    ax.legend(loc='best')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "from scipy.special import erf\n",
    "\n",
    "fig = plt.figure(figsize=(4*5, 4*2))\n",
    "axes = fig.subplots(2, 5)\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "y  = x \n",
    "dy = np.ones_like(x)\n",
    "plot(x, y, dy, 'Linear', axes[0, 0])\n",
    "\n",
    "y  = 1/(1+np.exp(-x)) \n",
    "dy = y*(1-y)\n",
    "plot(x, y, dy, 'Sigmoid', axes[0, 1])\n",
    "\n",
    "y  = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)) \n",
    "dy = 1-(y**2)\n",
    "plot(x, y, dy, 'Tanh', axes[0, 2])\n",
    "\n",
    "y  = np.maximum(x, 0) \n",
    "dy = np.heaviside(x,1) \n",
    "plot(x, y, dy, 'ReLU', axes[0, 3])\n",
    "\n",
    "alpha = 0.1\n",
    "y  = np.where(x<0, alpha*x, x)\n",
    "dy = np.where(x<0, alpha,   1)\n",
    "plot(x, y, dy, 'LeakyReLU', axes[0, 4])\n",
    "\n",
    "y  = np.heaviside(x,1) \n",
    "dy = np.zeros_like(y)\n",
    "plot(x, y, dy, 'Step', axes[1, 0])\n",
    "\n",
    "y  = np.log(1+np.exp(x))\n",
    "dy = 1/(1+np.exp(-x))\n",
    "plot(x, y, dy, 'Softplus', axes[1, 1])\n",
    "\n",
    "alpha = 2\n",
    "y  = np.where(x<=0, alpha*(np.exp(x)-1), x)\n",
    "dy = np.where(x<=0, alpha*np.exp(x), 1)\n",
    "plot(x, y, dy, 'ELU', axes[1, 2])\n",
    "\n",
    "f = 1 + np.exp(-x)\n",
    "y  = x/f\n",
    "dy = (f + (x*np.exp(-x)))/(f**2)\n",
    "plot(x, y, dy, 'Swish', axes[1, 3])\n",
    "\n",
    "s = x / np.sqrt(2)\n",
    "erf_prime = lambda x: (2 / np.sqrt(np.pi)) * np.exp(-(x ** 2))\n",
    "y  = 0.5 * x * (1 + erf(s))\n",
    "dy = 0.5 + 0.5 * erf(s) + ((0.5 * x * erf_prime(s)) / np.sqrt(2))\n",
    "plot(x, y, dy, 'GELU', axes[1, 4])\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    # Explictly make gradients 0.0\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    # List of Parameters\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # Initialises weights, bias and activations for the neuron\n",
    "    def __init__(self, nin, activation='ReLU', layer_name='', neuron_name=''):\n",
    "        \n",
    "        self.w = [Value(np.random.uniform(-1,1), label=f\"Weight of {layer_name} {neuron_name} for Input {i+1}\") for i in range(nin)]\n",
    "        self.b = Value(0, label=f\"Bias of {layer_name} {neuron_name}\")\n",
    "        self.activation = activation\n",
    "\n",
    "    # Sets the list of parameters in the neuron\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    # Information when printing neuron\n",
    "    def __repr__(self):\n",
    "        return f\"{self.activation}Neuron(nin={len(self.w)})\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the neuron\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Weighted Sum\n",
    "        w = sum((wi*xi for wi,xi in zip(self.w, x)))\n",
    "        \n",
    "        # Add the bias\n",
    "        out = w + self.b\n",
    "        \n",
    "        # Activation Function is applied\n",
    "        if self.activation == 'ReLU':\n",
    "            out = out.relu()\n",
    "        elif self.activation == 'Tanh':\n",
    "            out = out.tanh()\n",
    "        elif self.activation == 'Linear':\n",
    "            out = out\n",
    "            \n",
    "        # Output of Neuron\n",
    "        return out\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        # Define neurons of a layer\n",
    "        self.neurons = [Neuron(nin, neuron_name=f\"Neuron {i+1}\", **kwargs) for i in range(nout)]\n",
    "\n",
    "    # Sets the list of parameters in the layer\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    # Information when printing layer\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [ {', '.join(str(n) for n in self.neurons)} ]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the layer\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts, activations=None):\n",
    "        if activations is not None:\n",
    "            assert len(nouts) == len(activations), 'Activations not defined for some layers'\n",
    "        else:\n",
    "            activations = ['Linear'] * len(nouts)\n",
    "            \n",
    "        sz = [nin] + nouts \n",
    "        \n",
    "        # Define layers of a MLP\n",
    "        self.layers = [Layer(sz[i], sz[i+1], activation=activations[i], layer_name=f\"Layer {i+1}\") for i in range(len(nouts))]\n",
    "\n",
    "    # Sets the list of parameters in the MLP\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    # Information when printing MLP\n",
    "    def __repr__(self):\n",
    "        new_line = f\"\\n{'-'*8}> \"\n",
    "        return f\"MLP of [{new_line}{new_line.join(str(layer) for layer in self.layers)}\\n]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the MLP\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch_size=None, X=X_train, y=y_train):\n",
    "    \n",
    "    # Process Data in batches, in case data is too big to handle\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    \n",
    "    # Format Data to our Datatype\n",
    "    inputs = [ [Value(xrow[0], label='X'), Value(xrow[1], label='Y')] for xrow in Xb]\n",
    "    \n",
    "    # Forward Pass to get the scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Max-Margin Loss to calculate fitness based on scores and y\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    output_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # L2 Regularization (Optional)\n",
    "    ## To improve performance, we also regularise the parameters. \n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    \n",
    "    # Compute Final Loss -> Max-Margin Loss + L2 Regularization Loss\n",
    "    loss = output_loss + reg_loss\n",
    "    \n",
    "    # Compute Predictions and Accuracy\n",
    "    predictions = np.array([1 if (scorei.data > 0) else -1 for scorei in scores])\n",
    "    accuracy    = sum([(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)])/len(yb)\n",
    "    \n",
    "    # Return everything required\n",
    "    data = {}\n",
    "    data['loss']        = loss\n",
    "    data['scores']      = scores\n",
    "    data['predictions'] = predictions\n",
    "    data['accuracy']    = 100*accuracy\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(nin=2, nouts=[2, 2, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = compute_loss(model)\n",
    "print(f\"Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']: 5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scores'][0].backward()\n",
    "draw_dot(data['scores'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "n_log  = 1\n",
    "learning_rate = lr = 1.0\n",
    "\n",
    "model = MLP(nin=2, nouts=[16, 16, 1], activations=['ReLU', 'ReLU', 'Linear']) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "train_history = []\n",
    "\n",
    "# Optimize Iteratively\n",
    "for k in range(n_iter):\n",
    "    \n",
    "    # Zero-Grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward Pass -> Compute Loss\n",
    "    data = compute_loss(model)\n",
    "    \n",
    "    # Backward Pass\n",
    "    data['loss'].backward()\n",
    "        \n",
    "    # Log Details\n",
    "    if k % n_log == 0:\n",
    "        print(f\"Step: {k+1:3d} | Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']:5.2f}% | Learning Rate: {lr:.2f}\")\n",
    "        train_history.append((data['loss'].data, data['accuracy'], lr))\n",
    "    \n",
    "    # Update Weights using SGD\n",
    "    lr = learning_rate - 0.9*(k+1)/n_iter\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6*3, 6))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "train_history = np.asarray(train_history)\n",
    "\n",
    "for ax, d, t in zip(axes.flat, [train_history[:, 0], train_history[:, 1], train_history[:, 2]], ['Loss', 'Accuracy', 'Learning Rate']):\n",
    "    ax.plot(d)\n",
    "    ax.set(title=t, xlim=(0, n_iter))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = compute_loss(model, X=X_train, y=y_train)\n",
    "test_data  = compute_loss(model, X=X_test,  y=y_test)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Decision Boundary\n",
    "resolution = 0.25\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 \n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max+ resolution, resolution), np.arange(y_min, y_max+ resolution, resolution))\n",
    "\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "\n",
    "scores = list(map(model, inputs))\n",
    "\n",
    "Z = np.array([s.data > 0 for s in scores]).reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, colors=[cmap(-1%200), cmap(1)], alpha=0.25)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import repeat\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn, optim as optim\n",
    "from torchvision import datasets as tdatasets, transforms, utils as vutils\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cuda'\n",
    "\n",
    "out_dir = 'out'\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 128\n",
    "test_batch_size = 512\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs  = {'batch_size': test_batch_size}\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "            \n",
    "train_dataset = tdatasets.MNIST('../data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test_dataset  = tdatasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader  = infinite_loader(torch.utils.data.DataLoader(train_dataset,**train_kwargs))\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(vutils.make_grid(batch[0].to(device)[:32], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter          = 1000\n",
    "learning_rate   = 0.1\n",
    "n_log           = 1\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0.01, T_max=n_iter)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, n_iter):\n",
    "    train_history = []\n",
    "    model.train()\n",
    "    with tqdm(total=n_iter) as bar:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Explictily Zeroing Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % n_log == 0:\n",
    "                bar.update(n_log)\n",
    "                bar.set_postfix({'Loss':  f\"{loss.item():.4f}\", 'Learning Rate': f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
    "                train_history.append((loss.item, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            # Changing Learning Rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx == n_iter-1:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    \n",
    "    images    = []\n",
    "    labels    = []\n",
    "    outputs   = []\n",
    "    \n",
    "    with torch.no_grad() and tqdm(total=len(test_loader)) as bar:\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target, reduction='sum').item()  \n",
    "            \n",
    "            # Get Prediction\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            correct   += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss += loss\n",
    "            outputs.extend(output.detach().cpu().numpy())\n",
    "            images.extend(data.detach().cpu().numpy())\n",
    "            labels.extend(target.detach().cpu().numpy())\n",
    "            \n",
    "            bar.update(1)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct   /= len(test_loader.dataset)\n",
    "    images      = np.asarray(images).reshape(-1, 1, 28, 28).transpose(0, 2, 3, 1)\n",
    "    labels      = np.asarray(labels)\n",
    "    outputs     = np.asarray(outputs)\n",
    "\n",
    "    print(f\"Test set--- Average loss: {test_loss:.4f}, Accuracy: {100. * correct:.2f}%\")\n",
    "    return images, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, optimizer, scheduler, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(model).save(os.path.join(out_dir, 'mnist-ckpt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.jit.load(os.path.join(out_dir, 'mnist-ckpt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, outputs = test(trained_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(embedding, images, labels, title):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    ax  = fig.subplots()\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(embedding)\n",
    "    \n",
    "    for digit in range(10):\n",
    "        ax.scatter(*X[labels == digit].T, marker=f\"${digit}$\", s=60, color=plt.cm.Dark2(digit), alpha=0.425, zorder=2,)\n",
    "    \n",
    "    shown_images = np.array([[1.0, 1.0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(images[i], cmap=plt.cm.gray_r), X[i])\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "num_images = 1000\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2)\n",
    "\n",
    "image_embedding = tsne.fit_transform(images[:num_images].reshape(num_images, -1), labels[:num_images])\n",
    "plot_embedding(image_embedding, images[:num_images], labels[:num_images], \"Raw Data Representation\")\n",
    "\n",
    "output_embedding = tsne.fit_transform(outputs[:num_images].reshape(num_images, -1), labels[:num_images])\n",
    "plot_embedding(output_embedding, images[:num_images], labels[:num_images], \"Neural Network Representation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True \n",
    "torch.backends.cudnn.allow_tf32 = True \n",
    "\n",
    "device_type = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "out_dir = 'out'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/text'\n",
    "\n",
    "input_file_path = os.path.join(data_dir, 'input.txt')\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " n = len(data)\n",
    "\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data   = data[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = 50304 # Rounded to closest power of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Hello World\"\n",
    "print(f\"Phrase: {phrase:20s} {enc.encode_ordinary(phrase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Skip Skips Skipped\"\n",
    "print(f\"Phrase: {phrase:20s} {enc.encode_ordinary(phrase)}\")\n",
    "\n",
    "phrase = \"Laugh Laughs Laughed\"\n",
    "print(f\"Phrase: {phrase:20s} {enc.encode_ordinary(phrase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char2Int:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        chars = sorted(list(set(data)))\n",
    "        self.vocab_size = len(chars)\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    \n",
    "    def encode_ordinary(self, s):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, s):\n",
    "        return ''.join([self.itos[i] for i in l])\n",
    "\n",
    "enc = Char2Int(data)  \n",
    "vocab_size = enc.vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Hello World\"\n",
    "print(f\"Phrase: {phrase:20s} {enc.encode_ordinary(phrase)}\")\n",
    "\n",
    "phrase = \"Skip Skips Skipped\"\n",
    "print(f\"Phrase: {phrase:20s} {enc.encode_ordinary(phrase)}\")\n",
    "\n",
    "phrase = \"Laugh Laughs Laughed\"\n",
    "print(f\"Phrase: {phrase:20s} {enc.encode_ordinary(phrase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids   = enc.encode_ordinary(val_data)\n",
    "\n",
    "print(f\"Train Split has {len(train_ids):,} tokens\")\n",
    "print(f\"Val Split has {len(val_ids):,} tokens\")\n",
    "print(f\"Encoding Vocabulary Size is {vocab_size}\")\n",
    "\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids   = np.array(val_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(data_dir, 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "print(f\"Tokens per iteration will be: {batch_size * block_size}\")\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "        \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64))     for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias   = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head  = config.n_head\n",
    "        self.n_embd  = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # Query, Key, Value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # Batch size, Sequence length, Embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate Query, Key, Values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # Causal Self-Attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        ### Efficient attention using Flash Attention CUDA kernels\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        # Re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) \n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp  = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte  = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe  = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight \n",
    "\n",
    "        # Initialise all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report Number of Parameters\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        n_params -= self.transformer.wpe.weight.numel()    \n",
    "        print(\"Number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        device = idx.device\n",
    "        \n",
    "        b, t = idx.size()\n",
    "        \n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # Forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        \n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss   = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # If the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\\\n",
    "                \n",
    "            # Forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # Pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int    = 12\n",
    "    n_head: int     = 12\n",
    "    n_embd: int     = 768\n",
    "    dropout: float  = 0.0\n",
    "    bias: bool      = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 6 \n",
    "n_head  = 6 \n",
    "n_embd  = 384 \n",
    "bias    = False\n",
    "dropout = 0.0\n",
    "\n",
    "init_from = 'scratch' # 'scratch' or 'resume' \n",
    "\n",
    "model_args = dict(dropout=dropout)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    \n",
    "    model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size, bias=bias, vocab_size=vocab_size)    \n",
    "\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    \n",
    "    iteration     = 0\n",
    "    best_val_loss = 1e9\n",
    "\n",
    "elif init_from == 'resume':\n",
    "    \n",
    "    ckpt_path = os.path.join(out_dir, 'gpt-ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    \n",
    "    state_dict = checkpoint['model']\n",
    "\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "            \n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    iteration     = checkpoint['iteration']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "min_lr        = 1e-4           \n",
    "\n",
    "warmup_iters   = 100      \n",
    "lr_decay_iters = 5000  \n",
    "\n",
    "# Leraning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    \n",
    "    # 1) Linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    \n",
    "    # 2) If it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    \n",
    "    # 3) In between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + np.cos(np.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    \n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = np.linspace(0, 5000, 5000)\n",
    "plt.plot(iter, [get_lr(x) for x in iter])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a GradScaler.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# Start with all of the candidate parameters\n",
    "param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# Create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "# i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "decay_params   = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "optim_groups = [\n",
    "    {'params': decay_params,   'weight_decay': 0.1},\n",
    "    {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "print(f\"Num decayed parameter tensors: {len(decay_params)}, with {sum(p.numel() for p in decay_params)} parameters\")\n",
    "print(f\"Num non-decayed parameter tensors: {len(nodecay_params)}, with {sum(p.numel() for p in nodecay_params)} parameters\")\n",
    "\n",
    "# Create AdamW optimizer and use the fused version if it is available\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and device_type == 'cuda'\n",
    "\n",
    "# Initialize an optimizer\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), **extra_args)\n",
    "        \n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Compile the model\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Free up memory\n",
    "checkpoint = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in tqdm(range(eval_iters), desc=split, leave=False, position=1):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval    = 250\n",
    "n_log     = 10\n",
    "n_iter    = 5000 # total number of training iterations\n",
    "grad_clip = 0.0  # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# Converting data to required format\n",
    "X, Y = get_batch('train')\n",
    "         \n",
    "with tqdm(total=n_iter, position=0) as bar:\n",
    "    while True:\n",
    "\n",
    "        # Explictily Zeroing Gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Determine and set the learning rate for this iteration\n",
    "        lr = get_lr(iteration)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Forward Pass            \n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "        \n",
    "        # Converting data to required format\n",
    "        X, Y = get_batch('train')\n",
    "                \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "            \n",
    "        # Gradient Clipping\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "        # Updating Weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        if iteration % n_log == 0:\n",
    "            bar.update(n_log)\n",
    "            bar.set_postfix({'Loss':  f\"{loss.item():.4f}\"})            \n",
    "        \n",
    "        # Evaluate the loss on train/val sets and write checkpoints\n",
    "        if iteration % n_eval == 0 and iteration != 0:\n",
    "            losses = estimate_loss()\n",
    "            bar.set_description(f\"Iteration: {iteration} | Train loss {losses['train']:.4f} | Val loss {losses['val']:.4f} | \")\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                if iteration > 0:\n",
    "                    checkpoint = { \n",
    "                                  'model': model.state_dict(), \n",
    "                                  'optimizer': optimizer.state_dict(), \n",
    "                                  'model_args': model_args, \n",
    "                                  'iteration': iteration, \n",
    "                                  'best_val_loss': best_val_loss,\n",
    "                                }\n",
    "                    torch.save(checkpoint, os.path.join(out_dir, 'gpt-ckpt.pt'))\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Termination of Training\n",
    "        if iteration > n_iter:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"\\n\"\n",
    "start_ids = encode(start)\n",
    "start_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples    = 10\n",
    "max_new_tokens = 500\n",
    "temperature    = 0.8\n",
    "top_k          = 200\n",
    "\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "model.eval()\n",
    "with ctx:\n",
    "    for k in range(num_samples):\n",
    "        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print(enc.decode(y[0].tolist()))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install git+https://github.com/RobustBench/robustbench.git \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.optimize import differential_evolution as de\n",
    "\n",
    "import torch\n",
    "\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = load_cifar10(n_examples=50)\n",
    "model = load_model(model_name='Standard', dataset='cifar10', threat_model='Linf').to(device)\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 10\n",
    "image = x_test[image_id]\n",
    "label = y_test[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.softmax(model(image[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image(perturbations, img):\n",
    "    \n",
    "    perturbations = perturbations.astype(int)\n",
    "    if perturbations.ndim < 2:\n",
    "        perturbations = np.array([perturbations])\n",
    "    \n",
    "    img = (img * 255).detach().cpu().numpy().astype(int)\n",
    "    imgs = np.tile(img, [len(perturbations)] + [1]*(perturbations.ndim+1))\n",
    "    \n",
    "    for x, img in zip(perturbations, imgs):\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            x_pos, y_pos, *value = pixel\n",
    "            img[:, round(x_pos), round(y_pos)] = value\n",
    "        \n",
    "    imgs = imgs/255.0\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = np.array([6, 6, 255, 0, 0]) \n",
    "image_perturbed = perturb_image(pixel, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(image, target, pixel_count=1, maxiter=100, popsize=256):\n",
    "    \n",
    "    bounds = [(0,32), (0,32), (0,256), (0,256), (0,256)] * pixel_count\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "    \n",
    "    popsize = popmul * len(bounds)\n",
    "    nc = int(np.sqrt(popsize))\n",
    "    nr = int(np.ceil(popsize/float(nc)))\n",
    "\n",
    "    fig = plt.figure(figsize=(nc*4, nr*4))\n",
    "    axes = fig.subplots(nr, nc)\n",
    "    for ax, _ in zip(axes.flat, range(popsize)):\n",
    "        ax.set(title=f\"True Class: {classes[label]}\")\n",
    "        ax.set(xlabel=f\"Predicted Class {None}\\nTrue Class {None} has {None}% condifence\")\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "        ax.spines[['left', 'right', 'top', 'bottom']].set_visible(False)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "        \n",
    "    dh = display.display(fig, display_id=True)\n",
    "\n",
    "    \n",
    "    def run(perturbations, image, target, evaluate=False):\n",
    "        images_perturbed = perturb_image(perturbations, image).to(device)\n",
    "        probabilities = torch.softmax(model(images_perturbed), dim=1)\n",
    "        prediction = torch.argmax(probabilities, dim=1)\n",
    "        if evaluate:\n",
    "            return prediction != target\n",
    "        else:\n",
    "            confidence = probabilities[:, target].detach().cpu().numpy()\n",
    "            \n",
    "            for i, (ax, image, pred, fitness) in enumerate(zip(axes.flat, images_perturbed.permute(0, 2, 3, 1), prediction, confidence)):\n",
    "                ax.imshow(image)\n",
    "                ax.set(xlabel=f\"Predicted Class {classes[pred.item()]}\\nTrue Class {classes[label]} has {fitness*100:.2f}% condifence\")\n",
    "\n",
    "            dh.update(fig) \n",
    "            \n",
    "            return confidence\n",
    "    \n",
    "    def predict_fn(xs):\n",
    "        xs = xs.transpose()\n",
    "        return run(xs, image, target, evaluate=False)\n",
    "    \n",
    "    def callback_fn(x, convergence):\n",
    "        return run(x, image, target, evaluate=True)\n",
    "\n",
    "    result = de(\n",
    "        predict_fn, bounds=bounds, maxiter=maxiter, popsize=popmul, vectorized=True,\n",
    "        recombination=1, atol=-1, callback=callback_fn, polish=False, disp=True)\n",
    "    \n",
    "    return result.x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_perturbation = attack(image, label, pixel_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_perturbed = perturb_image(attacked_perturbation, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "true_conf = output[:, label]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\\nTrue Class {classes[label]} has {true_conf.item()*100:.2f}% condifence\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
