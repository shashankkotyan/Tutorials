{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install graphviz\n",
    "%pip install scikit-learn\n",
    "%pip install adjustText\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from graphviz import Digraph\n",
    "from adjustText import adjust_text\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a grid of Xs and Ys\n",
    "resolution = 100\n",
    "X, Y = np.meshgrid( np.linspace(-1,1,resolution), np.linspace(-1,1,resolution) )\n",
    "\n",
    "# Defining 4 different 2D functions\n",
    "mux, muy, sigma = 0.3, -0.3, 4\n",
    "G1 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = -0.3, 0.3, 2\n",
    "G2 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = 0.6, 0.6, 2\n",
    "G3 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux ,muy, sigma = -0.4, -0.2, 3\n",
    "G4 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "\n",
    "# Composing the final function\n",
    "G = G1 + G2 - G3 - G4\n",
    "\n",
    "\n",
    "fig  = plt.figure(figsize=(6*4,6)) # Defining the figure space\n",
    "axes = fig.subplots(1, 4)          # Defining the subplots in the figure\n",
    "\n",
    "for ax, g, t in zip(axes.flat, [G1, G2, G3, G4], ['G1', 'G2', 'G3', 'G4']): # Iterating over axes and functions\n",
    "    ax.imshow(g, vmin=-1, vmax=1, cmap='jet')                               # Ploting the function on the subplot\n",
    "    ax.set(title=t, xlim=(0, 100), ylim=(0, 100))                           # Setting the title and limits of the subplot\n",
    "\n",
    "fig.tight_layout() # Removes extra spacing from the figure\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cax = ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "ax.set(title=\"Function\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.colorbar(cax) # Attaching the colorbar to the figure\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()               # Instruct Matplotlib to show the figures created\n",
    "\n",
    "# fig.savefig(\"./assets/images/Optimization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5     # Number of Steps to take for optimisation\n",
    "alpha  = 0.03  # Learning rate of the optimisation\n",
    "\n",
    "w = np.array([70.0, 60.0]) # Starting Parameter (Point)\n",
    "sigma  = 3                 # Standard deviation of the samples around current parameter vector\n",
    "\n",
    "fig  = plt.figure( figsize=(5*n_iter, 5) )\n",
    "axes = fig.subplots(1, n_iter) \n",
    "\n",
    "prevx, prevy = [], []\n",
    "for q, ax in zip(range(n_iter), axes):\n",
    "    \n",
    "    # Draw the Optimization Landscape\n",
    "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "\n",
    "    # Sample Random Population\n",
    "    noise = np.random.randn(200, 2)\n",
    "    wp = np.expand_dims(w, 0) + sigma * noise\n",
    "    x,y = zip(*wp)\n",
    "    \n",
    "    # Estimate Gradient (Direction)\n",
    "    R  = np.array([G[int(wi[1]), int(wi[0])] for wi in wp])\n",
    "    R -= R.mean()\n",
    "    R /= R.std() \n",
    "    g  = np.dot(R, noise)\n",
    "    u  = alpha * g\n",
    "    \n",
    "    prevx.append(w[0])\n",
    "    prevy.append(w[1])\n",
    "    \n",
    "    # Draw Population on Landscape (Black Points)\n",
    "    ax.scatter(x, y, 4, 'k', edgecolors='face')\n",
    "    \n",
    "    # Draw estimated gradient (direction) as arrow (White Arrow)\n",
    "    ax.arrow(w[0], w[1], u[0], u[1], head_width=3, head_length=5, fc='w', ec='w')\n",
    "    \n",
    "    # Draw Parameter History (White Points)\n",
    "    ax.plot(prevx, prevy, 'wo-')\n",
    "    \n",
    "    # Update Parameter According to the gradient\n",
    "    w += alpha * g\n",
    "    \n",
    "    ax.set(title=f\"Iteration: {q+1} | Reward: {G[int(w[0]), int(w[1])]:.2f}\", xlim=(0, 100), ylim=(0, 100))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, label='', _children=(), _op=''):\n",
    "        \n",
    "        # Information about value, gradient and its name\n",
    "        self.data  = data\n",
    "        self.grad  = 0.0\n",
    "        self.label = label\n",
    "        \n",
    "        # Utility attributes for the calculating and passing gradients (Backprop)\n",
    "        self._backward = lambda: None\n",
    "        self._prev     = set(_children)\n",
    "        self._op       = _op \n",
    "    \n",
    "    # Simple arithemtic operations on value and computing corresponding gradients   \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, label='+', _children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, label='*', _children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad += self.data  * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, label=f'**{other}', _children=(self,), _op='**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Other arithmetic operations\n",
    "    ### Don't need to define backward functions since, they use __mul__ or __add__ for which backward is already defined. \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    # Simple transformations on Value and computing corresponding gradients\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, label='ReLU', _children=(self,), _op='ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, label='Tanh', _children=(self, ), _op='Tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "  \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), label='Exp',  _children=(self, ), _op='Exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Information when printing instance\n",
    "    def __repr__(self):\n",
    "        if self.label:\n",
    "            return f\"Value(node={self.label}, data={self.data}, grad={self.grad})\"\n",
    "        else:\n",
    "            return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # Recurisvely call backward -> Backprop\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the graph from a root node\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "# Visualizes the graph built from root node\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x):\n",
    "    return x**2 - 4*x + 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-7, 15, 100)\n",
    "Y = cost_function(X)\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(15.0, label='X')\n",
    "y = cost_function(x)\n",
    "\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3 # 0.1 # 0.3 # 0.9 #\n",
    "num_iterations = 10\n",
    "\n",
    "x = Value(15.0, label='X')\n",
    "\n",
    "xy_list = []\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Calculate f(x)\n",
    "    y = cost_function(x)\n",
    "    \n",
    "    # Calculate dy/dx\n",
    "    y.backward()\n",
    "    \n",
    "    xy_list.append((x.data, y.data))\n",
    "    print(f\"Step: {i+1:2d} | X: {x.data:5.2f} | f(X): {y.data:8.4f} | Gradient dy/dx: {x.grad:7.4f}\")\n",
    "    \n",
    "    # Update x \n",
    "    x -= alpha * x.grad\n",
    "\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "\n",
    "xy_list = np.asarray(xy_list)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax  = fig.subplots()\n",
    "\n",
    "ax.plot(X, Y)\n",
    "ax.plot(xy_list[:, 0], xy_list[:, 1], 'r--', marker=\"o\")\n",
    "\n",
    "texts = []\n",
    "for i in range(len(xy_list)):\n",
    "    text = ax.text(xy_list[i, 0], xy_list[i, 1], f\"({i+1}, {round(xy_list[i, 0], 2)}, {round(xy_list[i, 1], 4)})\", ha='center', va='center')\n",
    "    texts.append(text)\n",
    "adjust_text(texts, expand=(3, 3.5), arrowprops=dict(arrowstyle='->', color='grey'))\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Cost Function', title=f\"$f(x) = y = x^2 - 4x + 3$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = datasets.make_moons(n_samples=100, noise=0.1)\n",
    "X_test,  y_test  = datasets.make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "# make y be -1 or 1\n",
    "y_train = y_train*2 - 1 \n",
    "y_test  = y_test*2 - 1\n",
    " \n",
    "cmap = plt.cm.Spectral\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=[cmap(i%200) for i in y_train],               s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=[cmap(i%200) for i in y_test], c='w', s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# Weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# Bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, dy, title, ax):\n",
    "    ax.plot(x, y, linewidth=3, label=\"f(x)\", color=\"#69acc7\")\n",
    "    ax.plot(x, dy, linewidth=3, label=\"f'(x)\", color=\"#97c784\")\n",
    "    ax.set_title(f\"Curve for {title} with its derivative\")\n",
    "    ax.legend(loc='best')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "from scipy.special import erf\n",
    "\n",
    "fig = plt.figure(figsize=(4*5, 4*2))\n",
    "axes = fig.subplots(2, 5)\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "y  = x \n",
    "dy = np.ones_like(x)\n",
    "plot(x, y, dy, 'Linear', axes[0, 0])\n",
    "\n",
    "y  = 1/(1+np.exp(-x)) \n",
    "dy = y*(1-y)\n",
    "plot(x, y, dy, 'Sigmoid', axes[0, 1])\n",
    "\n",
    "y  = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)) \n",
    "dy = 1-(y**2)\n",
    "plot(x, y, dy, 'Tanh', axes[0, 2])\n",
    "\n",
    "y  = np.maximum(x, 0) \n",
    "dy = np.heaviside(x,1) \n",
    "plot(x, y, dy, 'ReLU', axes[0, 3])\n",
    "\n",
    "alpha = 0.1\n",
    "y  = np.where(x<0, alpha*x, x)\n",
    "dy = np.where(x<0, alpha,   1)\n",
    "plot(x, y, dy, 'LeakyReLU', axes[0, 4])\n",
    "\n",
    "y  = np.heaviside(x,1) \n",
    "dy = np.zeros_like(y)\n",
    "plot(x, y, dy, 'Step', axes[1, 0])\n",
    "\n",
    "y  = np.log(1+np.exp(x))\n",
    "dy = 1/(1+np.exp(-x))\n",
    "plot(x, y, dy, 'Softplus', axes[1, 1])\n",
    "\n",
    "alpha = 2\n",
    "y  = np.where(x<=0, alpha*(np.exp(x)-1), x)\n",
    "dy = np.where(x<=0, alpha*np.exp(x), 1)\n",
    "plot(x, y, dy, 'ELU', axes[1, 2])\n",
    "\n",
    "f = 1 + np.exp(-x)\n",
    "y  = x/f\n",
    "dy = (f + (x*np.exp(-x)))/(f**2)\n",
    "plot(x, y, dy, 'Swish', axes[1, 3])\n",
    "\n",
    "s = x / np.sqrt(2)\n",
    "erf_prime = lambda x: (2 / np.sqrt(np.pi)) * np.exp(-(x ** 2))\n",
    "y  = 0.5 * x * (1 + erf(s))\n",
    "dy = 0.5 + 0.5 * erf(s) + ((0.5 * x * erf_prime(s)) / np.sqrt(2))\n",
    "plot(x, y, dy, 'GELU', axes[1, 4])\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    # Explictly make gradients 0.0\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    # List of Parameters\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # Initialises weights, bias and activations for the neuron\n",
    "    def __init__(self, nin, activation='ReLU', layer_name='', neuron_name=''):\n",
    "        \n",
    "        self.w = [Value(np.random.uniform(-1,1), label=f\"Weight of {layer_name} {neuron_name} for Input {i+1}\") for i in range(nin)]\n",
    "        self.b = Value(0, label=f\"Bias of {layer_name} {neuron_name}\")\n",
    "        self.activation = activation\n",
    "\n",
    "    # Sets the list of parameters in the neuron\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    # Information when printing neuron\n",
    "    def __repr__(self):\n",
    "        return f\"{self.activation}Neuron(nin={len(self.w)})\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the neuron\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        w = sum((wi*xi for wi,xi in zip(self.w, x)))\n",
    "        out = w + self.b\n",
    "        \n",
    "        if self.activation == 'ReLU':\n",
    "            out = out.relu()\n",
    "        elif self.activation == 'Tanh':\n",
    "            out = out.tanh()\n",
    "        elif self.activation == 'Linear':\n",
    "            out = out\n",
    "            \n",
    "        return out\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        # Define neurons of a layer\n",
    "        self.neurons = [Neuron(nin, neuron_name=f\"Neuron {i+1}\", **kwargs) for i in range(nout)]\n",
    "\n",
    "    # Sets the list of parameters in the layer\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    # Information when printing layer\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [ {', '.join(str(n) for n in self.neurons)} ]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the layer\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts, activations=None):\n",
    "        if activations is not None:\n",
    "            assert len(nouts) == len(activations), 'Activations not defined for some layers'\n",
    "        else:\n",
    "            activations = ['Linear'] * len(nouts)\n",
    "            \n",
    "        sz = [nin] + nouts \n",
    "        \n",
    "        # Define layers of a MLP\n",
    "        self.layers = [Layer(sz[i], sz[i+1], activation=activations[i], layer_name=f\"Layer {i+1}\") for i in range(len(nouts))]\n",
    "\n",
    "    # Sets the list of parameters in the MLP\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    # Information when printing MLP\n",
    "    def __repr__(self):\n",
    "        new_line = f\"\\n{'-'*8}> \"\n",
    "        return f\"MLP of [{new_line}{new_line.join(str(layer) for layer in self.layers)}\\n]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the MLP\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch_size=None, X=X_train, y=y_train):\n",
    "    \n",
    "    # Process Data in batches, in case data is too big to handle\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    \n",
    "    # Format Data to our Datatype\n",
    "    inputs = [ [Value(xrow[0], label='X'), Value(xrow[1], label='Y')] for xrow in Xb]\n",
    "    \n",
    "    # Forward Pass to get the scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Max-Margin Loss to calculate fitness based on scores and y\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    output_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # L2 Regularization (Optional)\n",
    "    ## To improve performance, we also regularise the parameters. \n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    \n",
    "    # Compute Final Loss -> Max-Margin Loss + L2 Regularization Loss\n",
    "    loss = output_loss + reg_loss\n",
    "    \n",
    "    # Compute Predictions and Accuracy\n",
    "    predictions = np.array([1 if (scorei.data > 0) else -1 for scorei in scores])\n",
    "    accuracy    = sum([(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)])/len(yb)\n",
    "    \n",
    "    # Return everything required\n",
    "    data = {}\n",
    "    data['loss']        = loss\n",
    "    data['scores']      = scores\n",
    "    data['predictions'] = predictions\n",
    "    data['accuracy']    = 100*accuracy\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(nin=2, nouts=[2, 2, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = compute_loss(model)\n",
    "print(f\"Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']: 5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scores'][0].backward()\n",
    "draw_dot(data['scores'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "n_log  = 1\n",
    "learning_rate = lr = 1.0\n",
    "\n",
    "model = MLP(nin=2, nouts=[16, 16, 1], activations=['ReLU', 'ReLU', 'Linear']) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "train_history = []\n",
    "\n",
    "# Optimize Iteratively\n",
    "for k in range(n_iter):\n",
    "    \n",
    "    # Zero-Grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward Pass -> Compute Loss\n",
    "    data = compute_loss(model)\n",
    "    \n",
    "    # Backward Pass\n",
    "    data['loss'].backward()\n",
    "        \n",
    "    # Log Details\n",
    "    if k % n_log == 0:\n",
    "        print(f\"Step: {k+1:3d} | Loss: {data['loss'].data:.4f} | Accuracy: {data['accuracy']:5.2f}% | Learning Rate: {lr:.2f}\")\n",
    "        train_history.append((data['loss'].data, data['accuracy'], lr))\n",
    "    \n",
    "    # Update Weights using SGD\n",
    "    lr = learning_rate - 0.9*(k+1)/n_iter\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6*3, 6))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "train_history = np.asarray(train_history)\n",
    "\n",
    "for ax, d, t in zip(axes.flat, [train_history[:, 0], train_history[:, 1], train_history[:, 2]], ['Loss', 'Accuracy', 'Learning Rate']):\n",
    "    ax.plot(d)\n",
    "    ax.set(title=t, xlim=(0, n_iter))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = compute_loss(model, X=X_train, y=y_train)\n",
    "test_data  = compute_loss(model, X=X_test,  y=y_test)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(X_train[:, 0].min()-1, X_train[:, 0].max()+1), ylim=(X_train[:, 1].min()-1, X_train[:, 1].max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Decision Boundary\n",
    "resolution = 0.25\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 \n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max+ resolution, resolution), np.arange(y_min, y_max+ resolution, resolution))\n",
    "\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "\n",
    "scores = list(map(model, inputs))\n",
    "\n",
    "Z = np.array([s.data > 0 for s in scores]).reshape(xx.shape)\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.contourf(xx, yy, Z, colors=[cmap(-1%200), cmap(1)], alpha=0.25)\n",
    "\n",
    "train_color = [cmap(j%200) if i==j else 'k' for i, j in zip(train_data['predictions'], y_train)]\n",
    "test_color  = [cmap(j%200) if i==j else 'k' for i, j in zip(test_data['predictions'],  y_test)]\n",
    "\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=train_color,                s=20)\n",
    "ax.scatter(X_test[:,0],  X_test[:,1],  edgecolor=test_color, c='w',  s=20)\n",
    "\n",
    "ax.set(xlabel=\"X$_1$\", ylabel=\"X$_2$\", xlim =(x_min, x_max), ylim=(y_min, y_max))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import repeat\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn, optim as optim\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = 'cpu' if torch.cuda.is_available() 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 128\n",
    "test_batch_size = 512\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs  = {'batch_size': test_batch_size}\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "            \n",
    "train_dataset = datasets.MNIST('../data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test_dataset  = datasets.MNIST('../data', train=False, transform=transforms.ToTensor())\n",
    "train_loader  = infinite_loader(torch.utils.data.DataLoader(train_dataset,**train_kwargs))\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(vutils.make_grid(batch[0].to(device)[:32], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter          = 1000\n",
    "learning_rate   = 0.1\n",
    "n_log           = 1\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0.01, T_max=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, n_iter):\n",
    "    train_history = []\n",
    "    model.train()\n",
    "    with tqdm(total=n_iter) as bar:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Explictily Zeroing Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % n_log == 0:\n",
    "                bar.update(n_log)\n",
    "                bar.set_postfix({'Loss':  f\"{loss.item():.4f}\", 'Learning Rate': f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
    "                train_history.append((loss.item, scheduler.get_last_lr()[0]))\n",
    "            \n",
    "            # Changing Learning Rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx == n_iter-1:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    \n",
    "    images    = []\n",
    "    labels    = []\n",
    "    outputs   = []\n",
    "    \n",
    "    with torch.no_grad() and tqdm(total=len(test_loader)) as bar:\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target, reduction='sum').item()  \n",
    "            \n",
    "            # Get Prediction\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            correct   += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss += loss\n",
    "            outputs.extend(output.detach().cpu().numpy())\n",
    "            images.extend(data.detach().cpu().numpy())\n",
    "            labels.extend(target.detach().cpu().numpy())\n",
    "            \n",
    "            bar.update(1)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct   /= len(test_loader.dataset)\n",
    "    images      = np.asarray(images).reshape(-1, 1, 28, 28).transpose(0, 2, 3, 1)\n",
    "    labels      = np.asarray(labels)\n",
    "    outputs     = np.asarray(outputs)\n",
    "\n",
    "    print(f\"Test set--- Average loss: {test_loss:.4f}, Accuracy: {100. * correct:.2f}%\")\n",
    "    return images, labels, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, optimizer, scheduler, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(model).save('mnist-try.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.jit.load('mnist-try.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, outputs = test(trained_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(embedding, images, labels, title):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    ax  = fig.subplots()\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(embedding)\n",
    "    \n",
    "    for digit in range(10):\n",
    "        ax.scatter(*X[labels == digit].T, marker=f\"${digit}$\", s=60, color=plt.cm.Dark2(digit), alpha=0.425, zorder=2,)\n",
    "    \n",
    "    shown_images = np.array([[1.0, 1.0]])\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 4e-3:\n",
    "            continue\n",
    "        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(images[i], cmap=plt.cm.gray_r), X[i])\n",
    "        imagebox.set(zorder=1)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "num_images = 1000\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2)\n",
    "\n",
    "image_embedding = tsne.fit_transform(images[:num_images].reshape(num_images, -1), labels[:num_images])\n",
    "plot_embedding(image_embedding, images[:num_images], labels[:num_images], \"Raw Data Representation\")\n",
    "\n",
    "output_embedding = tsne.fit_transform(outputs[:num_images].reshape(num_images, -1), labels[:num_images])\n",
    "plot_embedding(output_embedding, images[:num_images], labels[:num_images], \"Neural Network Representation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/RobustBench/robustbench.git \n",
    "\n",
    "from scipy.optimize import differential_evolution as de\n",
    "\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = load_cifar10(n_examples=50)\n",
    "model = load_model(model_name='Standard', dataset='cifar10', threat_model='Linf').to(device)\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 10\n",
    "image = x_test[image_id]\n",
    "label = y_test[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.softmax(model(image[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image(perturbations, img):\n",
    "    \n",
    "    perturbations = perturbations.astype(int)\n",
    "    if perturbations.ndim < 2:\n",
    "        perturbations = np.array([perturbations])\n",
    "    \n",
    "    img = (img * 255).detach().cpu().numpy().astype(int)\n",
    "    imgs = np.tile(img, [len(perturbations)] + [1]*(perturbations.ndim+1))\n",
    "    \n",
    "    for x, img in zip(perturbations, imgs):\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            x_pos, y_pos, *value = pixel\n",
    "            img[:, round(x_pos), round(y_pos)] = value\n",
    "        \n",
    "    imgs = imgs/255.0\n",
    "    imgs = torch.from_numpy(imgs).float()\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = np.array([6, 6, 255, 0, 0]) \n",
    "image_perturbed = perturb_image(pixel, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(image, target, pixel_count=1, maxiter=100, popsize=256):\n",
    "    \n",
    "    bounds = [(0,32), (0,32), (0,256), (0,256), (0,256)] * pixel_count\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "    \n",
    "    def run(perturbations, image, target, evaluate=False):\n",
    "        images_perturbed = perturb_image(perturbations, image).to(device)\n",
    "        probabilities = torch.softmax(model(images_perturbed), dim=1)\n",
    "        if evaluate:\n",
    "            prediction = torch.argmax(probabilities, dim=1)\n",
    "            return prediction != target\n",
    "        else:\n",
    "            confidence = probabilities[:, target].detach().cpu().numpy()\n",
    "            return confidence\n",
    "    \n",
    "    def predict_fn(xs):\n",
    "        xs = xs.transpose()\n",
    "        return run(xs, image, target, evaluate=False)\n",
    "    \n",
    "    def callback_fn(x, convergence):\n",
    "        return run(x, image, target, evaluate=True)\n",
    "\n",
    "    result = de(\n",
    "        predict_fn, bounds=bounds, maxiter=maxiter, popsize=popmul, vectorized=True,\n",
    "        recombination=1, atol=-1, callback=callback_fn, polish=False, disp=True)\n",
    "    \n",
    "    return result.x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_perturbation = attack(image, label, pixel_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_perturbed = perturb_image(attacked_perturbation, image)[0]\n",
    "\n",
    "output = torch.softmax(model(image_perturbed[None, ...].to(device)), dim=1)\n",
    "conf, pred = output.max(dim=1)\n",
    "true_conf = output[:, label]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.imshow(image_perturbed.permute(1,2,0))\n",
    "ax.set(xticks=[], yticks=[], title=f\"True Class: {classes[label]}\", xlabel=f\"Predicted Class {classes[pred.item()]} has {conf.item()*100:.2f}% confidence\\nTrue Class {classes[label]} has {true_conf.item()*100:.2f}% condifence\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
