{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí´ Hands on learning of Deep Learning and Neural Networks üí´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/Introduction.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/Applications.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will help you understand the basics of neural networks from the perspective of optimization. \n",
    "At the end of the lecture, you will have an understanding on, \n",
    "1. Gradient Descent Algorithm for Optimization\n",
    "2. Modelling Simple Multi-Layer Perceptrons\n",
    "3. Backpropogation Algorithm\n",
    "4. Training your own simple neural network for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öíÔ∏è Let's setup by importing relevant libraries and relevant utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we import the relevant libraries. \n",
    "\n",
    "**Note**: Graphviz may require manual installation, see this webpage for more information (https://graphviz.org/download/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install graphviz\n",
    "%pip install scikit-learn\n",
    "%pip install adjustText\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets # import make_moons, make_blobs\n",
    "from graphviz import Digraph\n",
    "from adjustText import adjust_text\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚èÆÔ∏è Recap on Optimization: Playing with optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap on our optimization class. \n",
    "\n",
    "Suppose we have an optimization function that looks like this, \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/Optimization.png\" style=\"height: 500px\"/>\n",
    "</div>\n",
    "We want to create an **algorithm** that can find the maxima (or minima) of the given function, i.e., the reddest (or bluest) point in the function. \n",
    "\n",
    "Let's start by creating this toy function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a grid of Xs and Ys\n",
    "resolution = 100\n",
    "X, Y = np.meshgrid( np.linspace(-1,1,resolution), np.linspace(-1,1,resolution) )\n",
    "\n",
    "# Defining 4 different 2D functions\n",
    "mux, muy, sigma = 0.3, -0.3, 4\n",
    "G1 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = -0.3, 0.3, 2\n",
    "G2 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux, muy, sigma = 0.6, 0.6, 2\n",
    "G3 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "mux ,muy, sigma = -0.4, -0.2, 3\n",
    "G4 = np.exp(-((X-mux)**2+(Y-muy)**2)/2.0*sigma**2)\n",
    "\n",
    "# Composing the final function\n",
    "G = G1 + G2 - G3 - G4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6*4,6)) # Defining the figure space\n",
    "axes = fig.subplots(1, 4)         # Defining the subplots in the figure\n",
    "\n",
    "for ax, g, t in zip(axes.flat, [G1, G2, G3, G4], ['G1', 'G2', 'G3', 'G4']): # Iterating over axes and functions\n",
    "    ax.imshow(g, vmin=-1, vmax=1, cmap='jet')                               # Ploting the function on the subplot\n",
    "    ax.set(title=t)                                                          # Setting the title of the subplot\n",
    "\n",
    "fig.tight_layout() # Removes extra spacing from the figure\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.subplots()\n",
    "\n",
    "cax = ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "ax.set(title=\"G\")\n",
    "\n",
    "cbar = fig.colorbar(cax) # Attaching the colorbar to the figure\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()               # Instruct Matplotlib to show the figures created\n",
    "\n",
    "# fig.savefig(\"./assets/Optimization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the function, so we can start optimization on the function. \n",
    "\n",
    "Let's **start** at a point, (70.0, 60.0) on the grid.\n",
    "We will sample points around this region and the direction of movement with gradient.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5     # Number of Steps to take for optimisation\n",
    "alpha  = 0.03  # Learning rate of the optimisation\n",
    "\n",
    "w = np.array([70.0, 60.0]) # Starting Parameter (Point)\n",
    "sigma  = 3                 # Standard deviation of the samples around current parameter vector\n",
    "\n",
    "fig  = plt.figure( figsize=(5*n_iter, 5) )\n",
    "axes = fig.subplots(1, n_iter) \n",
    "\n",
    "prevx, prevy = [], []\n",
    "for q, ax in zip(range(n_iter), axes):\n",
    "    \n",
    "    # Draw the Optimization Landscape\n",
    "    ax.imshow(G, vmin=-1, vmax=1, cmap='jet')\n",
    "\n",
    "    # Sample Random Population\n",
    "    noise = np.random.randn(200, 2)\n",
    "    wp = np.expand_dims(w, 0) + sigma * noise\n",
    "    x,y = zip(*wp)\n",
    "    \n",
    "    # Estimate Gradient (Direction)\n",
    "    R  = np.array([G[int(wi[1]), int(wi[0])] for wi in wp])\n",
    "    R -= R.mean()\n",
    "    R /= R.std() \n",
    "    g  = np.dot(R, noise)\n",
    "    u  = alpha * g\n",
    "    \n",
    "    prevx.append(w[0])\n",
    "    prevy.append(w[1])\n",
    "    \n",
    "    # Draw Population on Landscape (Black Points)\n",
    "    ax.scatter(x, y, 4, 'k', edgecolors='face')\n",
    "    \n",
    "    # Draw estimated gradient (direction) as arrow (White Arrow)\n",
    "    ax.arrow(w[0], w[1], u[0], u[1], head_width=3, head_length=5, fc='w', ec='w')\n",
    "    \n",
    "    # Draw Parameter History (White Points)\n",
    "    ax.plot(prevx, prevy, 'wo-')\n",
    "    \n",
    "    # Update Parameter According to the gradient\n",
    "    w += u\n",
    "    \n",
    "    ax.set(title=f\"Iteration: {q+1} | Reward: {G[int(w[0]), int(w[1])]:.2f}\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üöÄ **Try-it-out**: Try finding minima bluest part of the figure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Playing with gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by creating our own **datatype** to save a single scalar value and its gradient. \n",
    "\n",
    "**Note**: We are creating our own datatype just to understand the behind the scenes working of well established libraries like, Numpy, Tensorflow, PyTorch, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, label='', _children=(), _op=''):\n",
    "        \n",
    "        # Information about value, gradient and its name\n",
    "        self.data  = data\n",
    "        self.grad  = 0.0\n",
    "        self.label = label\n",
    "        \n",
    "        # Utility attributes for the calculating and passing gradients (Backprop)\n",
    "        self._backward = lambda: None\n",
    "        self._prev     = set(_children)\n",
    "        self._op       = _op \n",
    "    \n",
    "    # Simple arithemtic operations on value and computing corresponding gradients   \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, label='+', _children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, label='*', _children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad += self.data  * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, label=f'**{other}', _children=(self,), _op='**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Other arithmetic operations\n",
    "    ### Don't need to define backward functions since, they use __mul__ or __add__ for which backward is already defined. \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    # Simple transformations on Value and computing corresponding gradients\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, label='ReLU', _children=(self,), _op='ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, label='Tanh', _children=(self, ), _op='Tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "  \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), label='Exp',  _children=(self, ), _op='Exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Information when printing instance\n",
    "    def __repr__(self):\n",
    "        if self.label:\n",
    "            return f\"Value(node={self.label}, data={self.data}, grad={self.grad})\"\n",
    "        else:\n",
    "            return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # Recurisvely call backward -> Backprop\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some visualisation utilities which will help us understand the flow of gradients and data in a complicated fucntion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the graph from a root node\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "# Visualisizes the graph built from root node\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining our own simple one-variable function and plot it. \n",
    "\n",
    "We will use a simple function, \n",
    "$$ f(x) = y = x^2 - 4x + 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x):\n",
    "    return x**2 - 4*x + 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-7, 15, 100)\n",
    "Y = cost_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(15.0, label='X')\n",
    "y = cost_function(x)\n",
    "\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know what is the gradient for the function:\n",
    "$$ \\frac{d(f(x))}{dx} = \\frac{dy}{dx} = 2x -4 $$ \n",
    "\n",
    "However, let's use a magic function of backprop where we do not explicitly calculate the gradient and let's see how it calculates the gradient. \n",
    "\n",
    "We know, that the gradient of the function should be: $$ 26 \\quad \\text{when} \\quad  x = 15.0 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the information from the gradient to update the x, can we find the minima of the function by just **iteratively** doing this updation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3 # 0.1 # 0.3 # 0.75 #\n",
    "num_iterations = 10\n",
    "\n",
    "x = Value(15.0, label='X')\n",
    "\n",
    "xy_list = []\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Calculate f(x)\n",
    "    y = cost_function(x)\n",
    "    \n",
    "    # Calculate dy/dx\n",
    "    y.backward()\n",
    "    \n",
    "    xy_list.append((x.data, y.data))\n",
    "    print(f\"Step: {i+1:2d} | X: {x.data:4.1f} | f(X): {y.data:6.2f} | Gradient dy/dx: {x.grad:6.2f}\")\n",
    "    \n",
    "    # Update x \n",
    "    x -= alpha * x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualise, what is happening with the x over the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_list = np.asarray(xy_list)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.plot(X, Y)\n",
    "ax.plot(xy_list[:, 0], xy_list[:, 1], 'r')\n",
    "\n",
    "for i in range(len(xy_list)):\n",
    "    ax.text(xy_list[i, 0], xy_list[i, 1] + 0.5, round(xy_list[i, 1], 2))\n",
    "\n",
    "    \n",
    "ax.set(xlabel='X', ylabel='Cost Function', title=f\"$f(x) = y = x^2 - 4x + 3$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üöÄ **Try-it-out**: Try some 2D function and see how it goes for the minima. \n",
    "#### üöÄ **Try-it-out**: Can you guess how gradients will be calculated manually for 2D functions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ What about classification ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have done a **regression task**, where we ride along the function to find the minima. \n",
    "\n",
    "However, what should be done when we want to perform **classification**?\n",
    "\n",
    "Moreover, what do we do when we have an approximation of the function defined by **sampled points**? \n",
    "\n",
    "Let's first visualise what I am talking about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "ax.set(xlabel=\"X\", ylabel=\"Y\", xlim =(X.min()-1, X.max()+1), ylim=(y.min()-1, y.max()+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the above figure, we don't have an exact defined function for either left-moon or right-more. Moreover, we have to separate the two moons rather than find a minima. \n",
    "\n",
    "<br>\n",
    "\n",
    "This is the task, which the current deep learning networks or artificial neural networks can do the best! \n",
    "\n",
    "Let's understand about the neural networks! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/ML-DL.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Microscopic view of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/Neuron.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Visualising Artificial Neuron (Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feed-Forward Neural Networks are inspired by the information processing of one or more neural cells, called a neuron. \n",
    "A neuron accepts input signals via its dendrites, which pass the electrical signal down to the cell body. \n",
    "The axon carries the signal out to synapses, which are the connections of a cell‚Äôs axon to other cell‚Äôs dendrites.\n",
    "\n",
    "<details>\n",
    "<br>\n",
    "The human nervous system is composed of more than 100 billion cells known as neurons. \n",
    "\n",
    "A neuron is a cell in the nervous system whose function it is to receive and transmit information. \n",
    "\n",
    "Neurons are made up of three major parts:\n",
    "\n",
    "* The cell body, or **soma**, which contains the nucleus of the cell and keeps the cell alive\n",
    "* A branching treelike fiber known as the **dendrite**, which collects information from other cells and sends the information to the soma\n",
    "* A long, segmented fiber known as the **axon**, which transmits information away from the cell body toward other neurons or to the muscles and glands\n",
    "\n",
    "<img src=\"https://c4.staticflickr.com/3/2656/4253587827_9723c3ffd3_z.jpg\" />\n",
    "\n",
    "*Photo courtesy of GE Healthcare, http://www.flickr.com/photos/gehealthcare/4253587827/ *\n",
    "\n",
    "<img src=\"https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg\"/>\n",
    "\n",
    "Some neurons have hundreds or even thousands of dendrites, and these dendrites may themselves be branched to allow the cell to receive information from thousands of other cells. \n",
    "\n",
    "The axons are also specialized; some, such as those that send messages from the spinal cord to the muscles in the hands or feet, may be very long---even up to several feet in length. \n",
    "To improve the speed of their communication, and to keep their electrical charges from shorting out with other neurons, axons are often surrounded by a **myelin sheath**. \n",
    "The myelin sheath is a layer of fatty tissue surrounding the axon of a neuron that both acts as an insulator and allows faster transmission of the electrical signal.\n",
    "Axons branch out toward their ends, and at the tip of each branch is a *terminal button*.\n",
    "\n",
    "</details>\n",
    "\n",
    "The actual working of neurons involves many aspects (including chemical, electrical, physical, timings). \n",
    "\n",
    "We will abstract all of this away into three numbers:\n",
    "\n",
    "* **Activation** - A value representing the excitement of a neuron\n",
    "* **Bias** - A value representing a default or bias (sometimes called a threshold)\n",
    "* **Weight** - A value representing a connection to another neuron\n",
    "\n",
    "In addition, there is a **transfer function** that takes all of the incoming activations times their associated weights plus the bias, and squashes the resulting sum. \n",
    "This limits the activations from growing too big or too small.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/Perceptron.png\"/>\n",
    "</div>\n",
    "\n",
    "A perceptron maintains an activation value that depends on the activation values of its incoming neighbors, the weights from its incoming neighbors, and an additional value, called the **default bias value**. To compute this activation value, we first calculate the node's net input.\n",
    "\n",
    "The net input is a weighted sum of all the incoming activations plus the node's bias value:\n",
    "\n",
    "$$ y = f(b + \\sum\\limits_{i=1}^n x_i w_i) $$\n",
    "\n",
    "where $w_{i}$ is the weight, or connection strength, from the $i^{th}$ node, $x_i$ is the  input, $b$ is the bias value, and $f$ is the activation function that transforms the linear combination of inputs and weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "\n",
    "# Weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "\n",
    "# Bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∏Ô∏è Network of Neurons (Multi Layer Perceptron --- MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To build a network of neurons, we first start by grouping neurons together in **layers**.\n",
    "\n",
    "A typical Artificial Neural Network (ANN) is composed of three layers: **input**, **hidden**, and **output**. Each layer contains a collection of neurons. Typically, the neurons in a layer are **fully connected** to the neurons in the next layer. \n",
    "\n",
    "For instance, every input neuron will have a weighted connection to every hidden neuron. Similarly, every hidden neuron will have a *weighted connection* to every output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"./assets/Lecture-ANN.png\"/>\n",
    "    <br>\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/NeuralNetwork.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing in a network works as follows:\n",
    "\n",
    "Input is propagated forward from the input layer through the hidden layer and finally through the output layer to produce a response. Each neuron, regardless of the layer it is in, uses the same transfer function in order to propagate its information forward to the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/ForwardPropagation.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's now create a neuron, a layer and a network of neurons using our defined Value class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    # Explictly make gradients 0.0\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    # List of Parameters\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, activation='ReLU', layer_name='', neuron_name=''):\n",
    "        \n",
    "        # Sets, weights, bias and activations for the neuron\n",
    "        self.w = [Value(np.random.uniform(-1,1), label=f\"Weight of {layer_name} {neuron_name} for Input {i+1}\") for i in range(nin)]\n",
    "        self.b = Value(0, label=f\"Bias of {layer_name} {neuron_name}\")\n",
    "        self.activation = activation\n",
    "\n",
    "    # Sets the list of parameters in the neuron\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    # Information when printing neuron\n",
    "    def __repr__(self):\n",
    "        return f\"{self.activation}Neuron(nin={len(self.w)})\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the neuron\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        w = sum((wi*xi for wi,xi in zip(self.w, x)))\n",
    "        out = w + self.b\n",
    "        \n",
    "        if self.activation == 'ReLU':\n",
    "            out = out.relu()\n",
    "        elif self.activation == 'Tanh':\n",
    "            out = out.tanh()\n",
    "        elif self.activation == 'Linear':\n",
    "            out = out\n",
    "            \n",
    "        return out\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        # Define neurons of a layer\n",
    "        self.neurons = [Neuron(nin, neuron_name=f\"Neuron {i+1}\", **kwargs) for i in range(nout)]\n",
    "\n",
    "    # Sets the list of parameters in the layer\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    # Information when printing layer\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [ {', '.join(str(n) for n in self.neurons)} ]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the layer\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts, activations=None):\n",
    "        if activations is not None:\n",
    "            assert len(nouts) == len(activations), 'Activations not defined for some layers'\n",
    "        else:\n",
    "            activations = ['Linear'] * len(nouts)\n",
    "            \n",
    "        sz = [nin] + nouts \n",
    "        \n",
    "        # Define layers of a MLP\n",
    "        self.layers = [Layer(sz[i], sz[i+1], activation=activations[i], layer_name=f\"Layer {i+1}\") for i in range(len(nouts))]\n",
    "\n",
    "    # Sets the list of parameters in the MLP\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    # Information when printing MLP\n",
    "    def __repr__(self):\n",
    "        new_line = f\"\\n{'-'*8}> \"\n",
    "        return f\"MLP of [{new_line}{new_line.join(str(layer) for layer in self.layers)}\\n]\"\n",
    "    \n",
    "    # Forward Pass -> Compute the output of the MLP\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function that will compute **loss** *(reward/penalty)* for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(batch_size=None):\n",
    "    \n",
    "    # Process Data in batches, in case data is too big to handle\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    \n",
    "    # Format Data to our Datatype\n",
    "    inputs = [ [Value(xrow[0], label='X'), Value(xrow[1], label='Y')] for xrow in Xb]\n",
    "    \n",
    "    # Forward Pass to get the scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Max-Margin Loss to calculate fitness based on scores and y\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    \n",
    "    # L2 Regularization (Optional)\n",
    "    ## To improve performance, we also regularise the parameters. \n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    \n",
    "    # Compute Final Loss -> Max-Margin Loss + L2 Regularization Loss\n",
    "    data_loss = data_loss + reg_loss\n",
    "    \n",
    "    # Compute Accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    \n",
    "    # Return everything required\n",
    "    return data_loss, scores, sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(nin=2, nouts=[2, 2, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, scores, acc = compute_loss()\n",
    "print(f\"Loss: {loss.data:.4f} | Accuracy: {100*acc: 5.2f}\")\n",
    "# print(f\"Prediction of 1st Input: {int(scores[1].data>0)} | True classification of 1st Input: {y[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ Backpropagation --- Trick to update weights (parameters) of multilayer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/Backpropagation.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "For many years, it was unknown how to learn the weights in a multi-layered neural network. In addition, Marvin Minsky and Seymour Papert proved in their 1969 book [Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book)) that you could not do simple functions without having multi-layers. \n",
    "\n",
    "(Actually, the idea of using simulated evolution to search for the weights could have been used, but no one thought to do that.) \n",
    "\n",
    "Specifically, they looked at the function XOR:\n",
    "\n",
    "**Input 1** | **Input 2** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 1 \n",
    " 1 | 0 | 1 \n",
    " 1 | 1 | 0 \n",
    "\n",
    "This killed research into neural networks for more than a decade. So, the idea of neural networks generally was ignored until the mid 1980s when the **Back-Propagation of Error** (Backprop) was created.\n",
    "</details>\n",
    "\n",
    "The **Backpropagation algorithm** (using *Backprop*), also called the *generalized delta rule*, is a *supervised* learning method for multilayer feed-forward networks in the field of Deep Learning.\n",
    "Technically, it is a method for training the weights in a multilayer feed-forward neural network. \n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected output signal. \n",
    "The system is trained using a supervised learning method, where the error between the system‚Äôs output and a known expected output is presented to the system and used to modify its internal state.\n",
    "\n",
    "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0].backward()\n",
    "draw_dot(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Train a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"480\" height=\"360\" src=\"./assets/Part6.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our neural network to classify the two moons. \n",
    "\n",
    "Backprop in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20\n",
    "n_log  = 1\n",
    "learning_rate = 1.0\n",
    "\n",
    "model = MLP(nin=2, nouts=[16, 16, 1], activations=['ReLU', 'ReLU', 'Linear']) # 2-layer neural network\n",
    "print(model)\n",
    "print(f\"Number of Parameters: {len(model.parameters())}\")\n",
    "print(f\"\\n{'-'*70}\\n\")\n",
    "data = []\n",
    "\n",
    "# Optimize Iteratively\n",
    "for k in range(n_iter):\n",
    "    \n",
    "    # Zero-Grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward Pass -> Compute Loss\n",
    "    loss, scores, acc = compute_loss()\n",
    "    \n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update Weights using SGD\n",
    "    lr = learning_rate - 0.9*(k+1)/n_iter\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    if k % n_log == 0:\n",
    "        print(f\"Step: {k+1:3d} | Loss: {loss.data:.4f} | Accuracy: {acc*100:3.2f}% | Learning Rate: {lr:.2f}\")\n",
    "        data.append((loss.data, acc, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualise, Loss, Accuracy and Learning Rate over the iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6*3, 6))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "data = np.asarray(data)\n",
    "\n",
    "for ax, d, t in zip(axes.flat, [data[:, 0], data[:, 1], data[:, 2]], ['Loss', 'Accuracy', 'Learning Rate']):\n",
    "    ax.plot(d)\n",
    "    ax.set(title=t, xlim=(0, n_iter))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the boundary of the trained neural network, how does the boundary separating the two moons look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Decision Boundary\n",
    "resolution = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution), np.arange(y_min, y_max, resolution))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "\n",
    "scores = list(map(model, inputs))\n",
    "\n",
    "Z = np.array([s.data > 0 for s in scores]).reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "ax.set(xlabel='X', ylabel='Y', xlim=(xx.min(), xx.max()), ylim=(yy.min(), yy.max()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üöÄ **Try-it-out**: Try some other simple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/Quiz-1.mp4\" controls></video>\n",
    "    <video width=\"1280\" height=\"720\" src=\"./assets/Quiz-2.mp4\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Moving Forward: Example of Handwritten Digits Classification using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a task, how does one train a neural network to do/solve the task? This involves the following steps:\n",
    "\n",
    "1. Determine an appropriate network architecture.\n",
    "2. Define a data set that will be used for training.\n",
    "3. Define the neural network parameters to be used for training.\n",
    "4. Train the network.\n",
    "5. Test the trained network.\n",
    "6. Do post training analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öíÔ∏è Libraries Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We again start by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import repeat\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn as nn, optim as optim\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèõÔ∏è Determining an appropriate architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a neural network consists of an input layer, an output layer, and zero or more hidden layers. Once a network has been trained, when you present an input to the network, the network will propagate the inputs through its layers to produce an output. \n",
    "\n",
    "If the input represents an instance of the task, the output should be the solution to that instance after the network has been trained. Thus, one can view a neural network as a general pattern associator. Thus, given a task, the first step is to identify the nature of inputs to the pattern associator. \n",
    "\n",
    "This is normally in the form of number of neurons required to represent the input. \n",
    "Similarly, you will need to determine how many output neurons will be required. \n",
    "\n",
    "For example, consider a simple logical connective, AND whose input-output characteristics are summarized in the table below:\n",
    "\n",
    "**Input A** | **Input B** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 0 \n",
    " 1 | 0 | 0 \n",
    " 1 | 1 | 1 \n",
    "\n",
    "This is a very simple example, but it will help us illustrate all of the important concepts in defining and training neural networks.\n",
    "\n",
    "In this example, it is clear that we will need two neurons in the input layer, and one in the output layer. \n",
    "We can start by assuming that we will not need a hidden layer. In general, as far as the design of a neural network is concerned, you always begin by identifying the size of the input and output layers. \n",
    "\n",
    "Then, you decide how many hidden layers you would use. \n",
    "In most situations you will need atleast one hidden layer, though there are no hard and fast rules about its size. \n",
    "Through much empirical practice, you will develop your own heuristics about this. \n",
    "\n",
    "For the MNIST Dataset, we have used 2 Convolution Layer and 2 MLP Layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé® Define a data set that will be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have decided on the network architecture, you have to prepare the data set that will be used for training. Each item in the data set represents an input pattern and the correct output pattern that should be produced by the network (since this is supervised training). \n",
    "\n",
    "In most tasks, there can be an infinite number of such input-output associations. Obviously it would be impossible to enumerate all associations for all tasks (and it would make little sense to even try to do this!). \n",
    "You have to then decide what comprises a good representative data set that, when used in training a network, would generalize to all situations.\n",
    "\n",
    "In the example of the AND, the data set is very small, finite (only 4 cases!), and **exhaustive**.\n",
    "\n",
    "However, MNIST Dataset consists of images of size 28*28 pixels which are monochromatic. Since monochromatic means either black or white, there exists $ 2^{28 \\times 28} $ possible images (this number is 237 digits long). If we go for greyscale, there would exist $ 256^{28\\times28} $ possible images (this number is 1889 digits long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size      = 128\n",
    "test_batch_size = 512\n",
    "n_iter          = 5000\n",
    "n_log           = 1\n",
    "device          = 'cpu'\n",
    "dry_run         = False\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs  = {'batch_size':  test_batch_size}\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        for data in loader:\n",
    "            yield data\n",
    "            \n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('../data', train=True,  download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('../data', train=False, transform=transform)\n",
    "train_loader  = infinite_loader(torch.utils.data.DataLoader(train_dataset,**train_kwargs))\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(torchvision.utils.make_grid(batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Define the neural network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the parameters required to train the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0.01, T_max=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ôªÔ∏è Training and Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once all the parameters are specified, you start the training process. \n",
    "This involves presenting each input pattern to the network, propagating it all the way until an output is produced, comparing the output with the desired target, computing the error, backpropagating the error, and applying the learning rule. \n",
    "This process is repeated as needed. \n",
    "\n",
    "In general, you should train the network for several iterations, it can be anywhere from a few hundred to millions! depennding on the dataset. \n",
    "Gradually, the network will begin to show improved and stable performance. \n",
    "Performance of the network is measured according to the task, for our classificiation task, we measure the accuracy, while for regression task, we can measure mean squared error. \n",
    "\n",
    "You can either stop the training process after a certain number of iterations have elapsed, or after the performance has saturated (early-stopping).\n",
    "\n",
    "Once the network has been trained, it is time to test it. \n",
    "There are several ways of doing this. \n",
    "Perhaps the easiest is to turn learning off and then see the outputs produced by the network for each input in the data set. \n",
    "When a trained network is going to be used in a *deployed* application, all you have to do is save the weights of all interconnections in the network into a file. \n",
    "The trained network can then be recreated at anytime by reloading the weights.\n",
    "\n",
    "**Note**: Instead of training-then-testing, there is another methodology: you can test-while-training, which encompases other evaluation technqiues like cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, n_iter):\n",
    "    model.train()\n",
    "    with tqdm(total=n_iter) as bar:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Explictily Zeroing Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % n_log == 0:\n",
    "                bar.update(n_log)\n",
    "                bar.set_postfix({'Loss':  f\"{loss.item():.4f}\", 'Learning Rate': f\"{scheduler.get_last_lr()[0]:.4f}\"})\n",
    "                if dry_run:\n",
    "                    break\n",
    "            \n",
    "            # Changing Learning Rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx == n_iter-1:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad() and tqdm(total=len(test_loader)) as bar:\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            # Converting data to required format\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = F.nll_loss(output, target, reduction='sum').item()  \n",
    "            \n",
    "            # Get Prediction\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            correct   += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss += loss\n",
    "            \n",
    "            bar.update(1)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct   /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {100. * correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, optimizer, scheduler, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Do post training analysis\n",
    "\n",
    "Perhaps the most important step in using neural networks is the analysis one performs once a network has been trained. There are a whole host of analysis techniques, here we present some of them which can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.imshow(np.transpose(torchvision.utils.make_grid(batch[0].to(device)[:16], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "ax.set(xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶â Words of Wisdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- You have to take into consideration the range of each input and output value. \n",
    "Remember the activation function of a neuron is a mostly sigmoid-like-function that serves to squash all input values between 0.0 and 1.0. \n",
    "Thus, regardless of the size of each input value into a node, the output produced by each node is between 0.0 and 1.0. \n",
    "This means that all output nodes have values in that range. I\n",
    "If the task you are dealing with expects outputs between 0.0 and 1.0, then there is nothing to worry about. \n",
    "However, in most situations, you will need to *scale* the output values back to the values in the task domain. \n",
    "Same goes for ReLU, which squashes the negative range. \n",
    "\n",
    "- In reality, it is also a good idea to scale the input values from the domain into the 0.0 to 1.0 range (especially if most input values are outside the -5.0 and 5.0 range). Thus, defining a data set for training almost always requires a collection of input-output pairs, as well as scaling and unscaling operations. Luckily, for the AND task, we do not need to do any scaling, but we will see several examples of this later.\n",
    "\n",
    "\n",
    "The learning rate, EPSILON, and the momentum constant, MOMENTUM, have to be between 0.0 and 1.0 and are critical to the overall training algorithm. \n",
    "The appropriate values of these constants are best determined by experimentation. \n",
    "Tolerance (which is also between 0.0 and 1.0) refers to the level of tolerance that is acceptable for determining correctness of the output. \n",
    "For example, if tolerance is set to 0.1, then an output value within 10% of the desired output is considered correct. \n",
    "Other training parameters generally exist to specify the reporting rate of the progress of the training, where to log such progress, etc. \n",
    "We will see specific examples of these as we start working with actual networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Extra Study Material\n",
    "\n",
    "- https://www.youtube.com/watch?v=aircAruvnKk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôè Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SimpliLearn- What is a Neural Network? https://www.youtube.com/watch?v=bfmFfD2RIcg\n",
    "- SimpliLearn- What is Deep Learning? https://www.youtube.com/watch?v=6M5VXKLf4D4\n",
    "- https://jupyter.brynmawr.edu/services/public/dblank/BioCS115%20Computing%20through%20Biology/2016-Spring/Notebooks/Artificial_Neural_Networks.ipynb\n",
    "- https://github.com/karpathy/micrograd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
